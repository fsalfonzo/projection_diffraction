{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRnQZWSZhArJ"
   },
   "source": [
    "# **1. Install Deep-STORM and dependencies**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSrZMo3X_NhO",
    "outputId": "60ff4707-b162-4d6e-f08d-48a053808872",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fpdf2 in /home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages (2.7.8)\n",
      "Requirement already satisfied: fonttools>=4.34.0 in /home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from fpdf2) (4.39.3)\n",
      "Requirement already satisfied: defusedxml in /home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from fpdf2) (9.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 15:44:49.431642: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024-04-20 15:44:51.372350: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-04-20 15:44:51.385097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.385182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 4090 computeCapability: 8.9\n",
      "coreClock: 2.55GHz coreCount: 128 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 938.86GiB/s\n",
      "2024-04-20 15:44:51.385195: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-04-20 15:44:51.449098: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-04-20 15:44:51.449226: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-04-20 15:44:51.456405: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-04-20 15:44:51.458520: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-04-20 15:44:51.461246: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-04-20 15:44:51.465014: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-04-20 15:44:51.466010: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-04-20 15:44:51.466094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.466207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.466518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2024-04-20 15:44:51.467092: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 15:44:51.468895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.468958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 4090 computeCapability: 8.9\n",
      "coreClock: 2.55GHz coreCount: 128 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 938.86GiB/s\n",
      "2024-04-20 15:44:51.468980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.469035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.469073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2024-04-20 15:44:51.469564: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-04-20 15:44:51.529683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-04-20 15:44:51.529702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2024-04-20 15:44:51.529705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2024-04-20 15:44:51.529884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.529969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.530028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-20 15:44:51.530088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22253 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "DeepSTORM installation complete.\n",
      "Notebook version: 1.13.1\n",
      "Latest notebook version: 1.13.3\n",
      "\u001b[31mA new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "Notebook_version = '1.13.1'\n",
    "Network = 'Deep-STORM'\n",
    "\n",
    "\n",
    "\n",
    "from builtins import any as b_any\n",
    "\n",
    "def get_requirements_path():\n",
    "    # Store requirements file in 'contents' directory \n",
    "    current_dir = os.getcwd()\n",
    "    dir_count = current_dir.count('/') - 1\n",
    "    path = 'requirements.txt'\n",
    "    return path\n",
    "\n",
    "def filter_files(file_list, filter_list):\n",
    "    filtered_list = []\n",
    "    for fname in file_list:\n",
    "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
    "            filtered_list.append(fname)\n",
    "    return filtered_list\n",
    "\n",
    "def build_requirements_file(before, after):\n",
    "    path = get_requirements_path()\n",
    "\n",
    "    # Exporting requirements.txt for local run\n",
    "    !pip freeze > $path\n",
    "\n",
    "    # Get minimum requirements file\n",
    "    df = pd.read_csv(path)\n",
    "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
    "    req_list_temp = df.values.tolist()\n",
    "    req_list = [x[0] for x in req_list_temp]\n",
    "\n",
    "    # Replace with package name and handle cases where import name is different to module name\n",
    "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
    "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list] \n",
    "    filtered_list = filter_files(req_list, mod_replace_list)\n",
    "\n",
    "    file=open(path,'w')\n",
    "    for item in filtered_list:\n",
    "        file.writelines(item)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "import sys\n",
    "before = [str(m) for m in sys.modules]\n",
    "\n",
    "#@markdown ##Install Deep-STORM and dependencies\n",
    "# %% Model definition + helper functions\n",
    "\n",
    "!pip install fpdf2\n",
    "# Import keras modules and libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation, UpSampling2D, Convolution2D, MaxPooling2D, BatchNormalization, Layer\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers, losses\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from skimage.transform import warp\n",
    "from skimage.transform import SimilarityTransform\n",
    "from skimage.metrics import structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "# Import common libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from os.path import abspath\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "from PIL import Image \n",
    "from PIL.TiffTags import TAGS\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from astropy.visualization import simple_norm\n",
    "from sys import getsizeof\n",
    "from fpdf import FPDF, HTMLMixin\n",
    "from fpdf.enums import XPos, YPos\n",
    "from pip._internal.operations.freeze import freeze\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# For sliders and dropdown menu, progress bar\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For Multi-threading in simulation\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "\n",
    "# define a function that projects and rescales an image to the range [0,1]\n",
    "def project_01(im):\n",
    "    im = np.squeeze(im)\n",
    "    min_val = im.min()\n",
    "    max_val = im.max()\n",
    "    return (im - min_val)/(max_val - min_val)\n",
    "\n",
    "# normalize image given mean and std\n",
    "def normalize_im(im, dmean, dstd):\n",
    "    im = np.squeeze(im)\n",
    "    im_norm = np.zeros(im.shape,dtype=np.float32)\n",
    "    im_norm = (im - dmean)/dstd\n",
    "    return im_norm\n",
    "\n",
    "# Define the loss history recorder\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "#  Define a matlab like gaussian 2D filter\n",
    "def matlab_style_gauss2D(shape=(7,7),sigma=1):\n",
    "    \"\"\" \n",
    "    2D gaussian filter - should give the same result as:\n",
    "    MATLAB's fspecial('gaussian',[shape],[sigma]) \n",
    "    \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h.astype(dtype=K.floatx())\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    h = h*2.0\n",
    "    h = h.astype('float32')\n",
    "    return h\n",
    "\n",
    "# Expand the filter dimensions\n",
    "psf_heatmap = matlab_style_gauss2D(shape = (7,7),sigma=1)\n",
    "gfilter = tf.reshape(psf_heatmap, [7, 7, 1, 1])\n",
    "\n",
    "# Combined MSE + L1 loss\n",
    "def L1L2loss(input_shape):\n",
    "    def bump_mse(heatmap_true, spikes_pred):\n",
    "\n",
    "        # generate the heatmap corresponding to the predicted spikes\n",
    "        heatmap_pred = K.conv2d(spikes_pred, gfilter, strides=(1, 1), padding='same')\n",
    "\n",
    "        # heatmaps MSE\n",
    "        loss_heatmaps = losses.mean_squared_error(heatmap_true,heatmap_pred)\n",
    "\n",
    "        # l1 on the predicted spikes\n",
    "        loss_spikes = losses.mean_absolute_error(spikes_pred,tf.zeros(input_shape))\n",
    "        return loss_heatmaps + loss_spikes\n",
    "    return bump_mse\n",
    "\n",
    "# Define the concatenated conv2, batch normalization, and relu block\n",
    "def conv_bn_relu(nb_filter, rk, ck, name):\n",
    "    def f(input):\n",
    "        conv = Convolution2D(nb_filter, kernel_size=(rk, ck), strides=(1,1),\\\n",
    "                               padding=\"same\", use_bias=False,\\\n",
    "                               kernel_initializer=\"Orthogonal\",name='conv-'+name)(input)\n",
    "        conv_norm = BatchNormalization(name='BN-'+name)(conv)\n",
    "        conv_norm_relu = Activation(activation = \"relu\",name='Relu-'+name)(conv_norm)\n",
    "        return conv_norm_relu\n",
    "    return f\n",
    "\n",
    "# Define the model architechture\n",
    "def CNN(input,names):\n",
    "    Features1 = conv_bn_relu(32,3,3,names+'F1')(input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2),name=names+'Pool1')(Features1)\n",
    "    Features2 = conv_bn_relu(64,3,3,names+'F2')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool2')(Features2)\n",
    "    Features3 = conv_bn_relu(128,3,3,names+'F3')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool3')(Features3)\n",
    "    Features4 = conv_bn_relu(512,3,3,names+'F4')(pool3)\n",
    "    up5 = UpSampling2D(size=(2, 2),name=names+'Upsample1')(Features4)\n",
    "    Features5 = conv_bn_relu(128,3,3,names+'F5')(up5)\n",
    "    up6 = UpSampling2D(size=(2, 2),name=names+'Upsample2')(Features5)\n",
    "    Features6 = conv_bn_relu(64,3,3,names+'F6')(up6)\n",
    "    up7 = UpSampling2D(size=(2, 2),name=names+'Upsample3')(Features6)\n",
    "    Features7 = conv_bn_relu(32,3,3,names+'F7')(up7)\n",
    "    return Features7\n",
    "\n",
    "# Define the Model building for an arbitrary input size\n",
    "def buildModel(input_dim, initial_learning_rate = 0.001):\n",
    "    input_ = Input (shape = (input_dim))\n",
    "    act_ = CNN (input_,'CNN')\n",
    "    density_pred = Convolution2D(1, kernel_size=(1, 1), strides=(1, 1), padding=\"same\",\\\n",
    "                                  activation=\"linear\", use_bias = False,\\\n",
    "                                  kernel_initializer=\"Orthogonal\",name='Prediction')(act_)\n",
    "    model = Model (inputs= input_, outputs=density_pred)\n",
    "    opt = optimizers.Adam(lr = initial_learning_rate)\n",
    "    model.compile(optimizer=opt, loss = L1L2loss(input_dim))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define a function that trains a model for a given data SNR and density\n",
    "def train_model(patches, heatmaps, modelPath, epochs, steps_per_epoch, batch_size, upsampling_factor=8, validation_split = 0.3, initial_learning_rate = 0.001, pretrained_model_path = '', L2_weighting_factor = 100):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains a CNN model on the desired training set, given the \n",
    "    upsampled training images and labels generated in MATLAB.\n",
    "    \n",
    "    # Inputs\n",
    "    # TO UPDATE ----------\n",
    "\n",
    "    # Outputs\n",
    "    function saves the weights of the trained model to a hdf5, and the \n",
    "    normalization factors to a mat file. These will be loaded later for testing \n",
    "    the model in test_model.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # for reproducibility\n",
    "    np.random.seed(123)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(patches, heatmaps, test_size = validation_split, random_state=42)\n",
    "    print('Number of training examples: %d' % X_train.shape[0])\n",
    "    print('Number of validation examples: %d' % X_test.shape[0])\n",
    "       \n",
    "    # Setting type\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "\n",
    "    \n",
    "    #===================== Training set normalization ==========================\n",
    "    # normalize training images to be in the range [0,1] and calculate the \n",
    "    # training set mean and std\n",
    "    mean_train = np.zeros(X_train.shape[0],dtype=np.float32)\n",
    "    std_train = np.zeros(X_train.shape[0], dtype=np.float32)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train[i, :, :] = project_01(X_train[i, :, :])\n",
    "        mean_train[i] = X_train[i, :, :].mean()\n",
    "        std_train[i] = X_train[i, :, :].std()\n",
    "\n",
    "    # resulting normalized training images\n",
    "    mean_val_train = mean_train.mean()\n",
    "    std_val_train = std_train.mean()\n",
    "    X_train_norm = np.zeros(X_train.shape, dtype=np.float32)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train_norm[i, :, :] = normalize_im(X_train[i, :, :], mean_val_train, std_val_train)\n",
    "    \n",
    "    # patch size\n",
    "    psize = X_train_norm.shape[1]\n",
    "\n",
    "    # Reshaping\n",
    "    X_train_norm = X_train_norm.reshape(X_train.shape[0], psize, psize, 1)\n",
    "\n",
    "    # ===================== Test set normalization ==========================\n",
    "    # normalize test images to be in the range [0,1] and calculate the test set \n",
    "    # mean and std\n",
    "    mean_test = np.zeros(X_test.shape[0],dtype=np.float32)\n",
    "    std_test = np.zeros(X_test.shape[0], dtype=np.float32)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test[i, :, :] = project_01(X_test[i, :, :])\n",
    "        mean_test[i] = X_test[i, :, :].mean()\n",
    "        std_test[i] = X_test[i, :, :].std()\n",
    "\n",
    "    # resulting normalized test images\n",
    "    mean_val_test = mean_test.mean()\n",
    "    std_val_test = std_test.mean()\n",
    "    X_test_norm = np.zeros(X_test.shape, dtype=np.float32)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test_norm[i, :, :] = normalize_im(X_test[i, :, :], mean_val_test, std_val_test)\n",
    "        \n",
    "    # Reshaping\n",
    "    X_test_norm = X_test_norm.reshape(X_test.shape[0], psize, psize, 1)\n",
    "\n",
    "    # Reshaping labels\n",
    "    Y_train = y_train.reshape(y_train.shape[0], psize, psize, 1)\n",
    "    Y_test = y_test.reshape(y_test.shape[0], psize, psize, 1)\n",
    "\n",
    "    # Save datasets to a matfile to open later in matlab\n",
    "    mdict = {\"mean_test\": mean_val_test, \"std_test\": std_val_test, \"upsampling_factor\": upsampling_factor, \"Normalization factor\": L2_weighting_factor}\n",
    "    sio.savemat(os.path.join(modelPath,\"model_metadata.mat\"), mdict)\n",
    "\n",
    "\n",
    "    # Set the dimensions ordering according to tensorflow consensous\n",
    "    # K.set_image_dim_ordering('tf')\n",
    "    K.set_image_data_format('channels_last')\n",
    "\n",
    "    # Save the model weights after each epoch if the validation loss decreased\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(modelPath,\"weights_best.hdf5\"), verbose=1,\n",
    "                                   save_best_only=True)\n",
    "\n",
    "    # Change learning when loss reaches a plataeu\n",
    "    change_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00005)\n",
    "    \n",
    "    # Model building and complitation\n",
    "    model = buildModel((psize, psize, 1), initial_learning_rate = initial_learning_rate)\n",
    "    model.summary()\n",
    "\n",
    "    # Load pretrained model\n",
    "    if not pretrained_model_path:\n",
    "      print('Using random initial model weights.')\n",
    "    else:\n",
    "      print('Loading model weights from '+pretrained_model_path)\n",
    "      model.load_weights(pretrained_model_path)\n",
    "    \n",
    "    # Create an image data generator for real time data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0.,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.,  # randomly shift images vertically (fraction of total height)\n",
    "        zoom_range=0.,\n",
    "        shear_range=0.,\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        fill_mode='constant',\n",
    "        data_format=K.image_data_format())\n",
    "\n",
    "    # Fit the image generator on the training data\n",
    "    datagen.fit(X_train_norm)\n",
    "    \n",
    "    # loss history recorder\n",
    "    history = LossHistory()\n",
    "\n",
    "    # Inform user training begun\n",
    "    print('-------------------------------')\n",
    "    print('Training model...')\n",
    "\n",
    "    # Fit model on the batches generated by datagen.flow()\n",
    "    train_history = model.fit_generator(datagen.flow(X_train_norm, Y_train, batch_size=batch_size), \n",
    "                                        steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n",
    "                                        validation_data=(X_test_norm, Y_test), \n",
    "                                        callbacks=[history, checkpointer, change_lr])    \n",
    "\n",
    "    # Inform user training ended\n",
    "    print('-------------------------------')\n",
    "    print('Training Complete!')\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(modelPath, 'weights_last.hdf5'))\n",
    "\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    lossData = pd.DataFrame(train_history.history) \n",
    "\n",
    "    if os.path.exists(os.path.join(modelPath,\"Quality Control\")):\n",
    "      shutil.rmtree(os.path.join(modelPath,\"Quality Control\"))\n",
    "\n",
    "    os.makedirs(os.path.join(modelPath,\"Quality Control\"))\n",
    "\n",
    "    # The training evaluation.csv is saved (overwrites the Files if needed). \n",
    "    lossDataCSVpath = os.path.join(modelPath,\"Quality Control/training_evaluation.csv\")\n",
    "    with open(lossDataCSVpath, 'w') as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerow(['loss','val_loss','learning rate'])\n",
    "      for i in range(len(train_history.history['loss'])):\n",
    "        writer.writerow([train_history.history['loss'][i], train_history.history['val_loss'][i], train_history.history['lr'][i]])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Normalization functions from Martin Weigert used in CARE\n",
    "def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "    \"\"\"Percentile-based image normalization.\"\"\"\n",
    "\n",
    "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
    "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
    "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
    "\n",
    "\n",
    "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "    if dtype is not None:\n",
    "        x   = x.astype(dtype,copy=False)\n",
    "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
    "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
    "        eps = dtype(eps)\n",
    "\n",
    "    try:\n",
    "        import numexpr\n",
    "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
    "    except ImportError:\n",
    "        x =                   (x - mi) / ( ma - mi + eps )\n",
    "\n",
    "    if clip:\n",
    "        x = np.clip(x,0,1)\n",
    "\n",
    "    return x\n",
    "\n",
    "def norm_minmse(gt, x, normalize_gt=True):\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    normalizes and affinely scales an image pair such that the MSE is minimized  \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    gt: ndarray\n",
    "        the ground truth image      \n",
    "    x: ndarray\n",
    "        the image that will be affinely scaled \n",
    "    normalize_gt: bool\n",
    "        set to True of gt image should be normalized (default)\n",
    "    Returns\n",
    "    -------\n",
    "    gt_scaled, x_scaled \n",
    "    \"\"\"\n",
    "    if normalize_gt:\n",
    "        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n",
    "    x = x.astype(np.float32, copy=False) - np.mean(x)\n",
    "    #x = x - np.mean(x)\n",
    "    gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n",
    "    #gt = gt - np.mean(gt)\n",
    "    scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n",
    "    return gt, scale * x\n",
    "\n",
    "\n",
    "# Multi-threaded Erf-based image construction\n",
    "@njit(parallel=True)\n",
    "def FromLoc2Image_Erf(xc_array, yc_array, photon_array, sigma_array, image_size = (64,64), pixel_size = 100):\n",
    "  w = image_size[0]\n",
    "  h = image_size[1]\n",
    "  erfImage = np.zeros((w, h))\n",
    "  for ij in prange(w*h):\n",
    "    j = int(ij/w)\n",
    "    i = ij - j*w\n",
    "    for (xc, yc, photon, sigma) in zip(xc_array, yc_array, photon_array, sigma_array):\n",
    "      # Don't bother if the emitter has photons <= 0 or if Sigma <= 0\n",
    "      if (sigma > 0) and (photon > 0):\n",
    "        S = sigma*math.sqrt(2)\n",
    "        x = i*pixel_size - xc\n",
    "        y = j*pixel_size - yc\n",
    "        # Don't bother if the emitter is further than 4 sigma from the centre of the pixel\n",
    "        if (x+pixel_size/2)**2 + (y+pixel_size/2)**2 < 16*sigma**2:\n",
    "          ErfX = math.erf((x+pixel_size)/S) - math.erf(x/S)\n",
    "          ErfY = math.erf((y+pixel_size)/S) - math.erf(y/S)\n",
    "          erfImage[j][i] += 0.25*photon*ErfX*ErfY\n",
    "  return erfImage\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = (64,64), pixel_size = 100):\n",
    "  w = image_size[0]\n",
    "  h = image_size[1]\n",
    "  locImage = np.zeros((image_size[0],image_size[1]) )\n",
    "  n_locs = len(xc_array)\n",
    "\n",
    "  for e in prange(n_locs):\n",
    "    locImage[int(max(min(round(yc_array[e]/pixel_size),w-1),0))][int(max(min(round(xc_array[e]/pixel_size),h-1),0))] += 1\n",
    "\n",
    "  return locImage\n",
    "\n",
    "\n",
    "\n",
    "def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n",
    "  with Image.open(TIFFpath) as img:\n",
    "    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag.keys()}\n",
    "\n",
    "\n",
    "  # TIFF tags\n",
    "  # https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml\n",
    "  # https://www.awaresystems.be/imaging/tiff/tifftags/resolutionunit.html\n",
    "  ResolutionUnit = meta_dict['ResolutionUnit'][0] # unit of resolution\n",
    "  width = meta_dict['ImageWidth'][0]\n",
    "  height = meta_dict['ImageLength'][0]\n",
    "\n",
    "  xResolution = meta_dict['XResolution'][0] # number of pixels / ResolutionUnit\n",
    "\n",
    "  if len(xResolution) == 1:\n",
    "    xResolution = xResolution[0]\n",
    "  elif len(xResolution) == 2:\n",
    "    xResolution = xResolution[0]/xResolution[1]\n",
    "  else:\n",
    "    print('Image resolution not defined.')\n",
    "    xResolution = 1\n",
    "\n",
    "  if ResolutionUnit == 2:\n",
    "    # Units given are in inches\n",
    "    pixel_size = 0.025*1e9/xResolution\n",
    "  elif ResolutionUnit == 3:\n",
    "    # Units given are in cm\n",
    "    pixel_size = 0.01*1e9/xResolution\n",
    "  else: \n",
    "    # ResolutionUnit is therefore 1\n",
    "    print('Resolution unit not defined. Assuming: um')\n",
    "    pixel_size = 1e3/xResolution\n",
    "\n",
    "  if display:\n",
    "    print('Pixel size obtained from metadata: '+str(pixel_size)+' nm')\n",
    "    print('Image size: '+str(width)+'x'+str(height))\n",
    "  \n",
    "  return (pixel_size, width, height)\n",
    "\n",
    "\n",
    "def saveAsTIF(path, filename, array, pixel_size):\n",
    "  \"\"\"\n",
    "  Image saving using PIL to save as .tif format\n",
    "  # Input \n",
    "  path       - path where it will be saved\n",
    "  filename   - name of the file to save (no extension)\n",
    "  array      - numpy array conatining the data at the required format\n",
    "  pixel_size - physical size of pixels in nanometers (identical for x and y)\n",
    "  \"\"\"\n",
    "\n",
    "  # print('Data type: '+str(array.dtype))\n",
    "  if (array.dtype == np.uint16):\n",
    "    mode = 'I;16'\n",
    "  elif (array.dtype == np.uint32):\n",
    "    mode = 'I'\n",
    "  else:\n",
    "    mode = 'F'\n",
    "\n",
    "  # Rounding the pixel size to the nearest number that divides exactly 1cm.\n",
    "  # Resolution needs to be a rational number --> see TIFF format\n",
    "  # pixel_size = 10000/(round(10000/pixel_size))\n",
    "\n",
    "  if len(array.shape) == 2:\n",
    "    im = Image.fromarray(array)\n",
    "    im.save(os.path.join(path, filename+'.tif'),\n",
    "                  mode = mode,  \n",
    "                  resolution_unit = 3,\n",
    "                  resolution = 0.01*1e9/pixel_size)\n",
    "\n",
    "\n",
    "  elif len(array.shape) == 3:\n",
    "    imlist = []\n",
    "    for frame in array:\n",
    "      imlist.append(Image.fromarray(frame))\n",
    "\n",
    "    imlist[0].save(os.path.join(path, filename+'.tif'), save_all=True,\n",
    "                  append_images=imlist[1:],\n",
    "                  mode = mode,  \n",
    "                  resolution_unit = 3,\n",
    "                  resolution = 0.01*1e9/pixel_size)\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Maximafinder(Layer):\n",
    "    def __init__(self, thresh, neighborhood_size, use_local_avg, **kwargs):\n",
    "        super(Maximafinder, self).__init__(**kwargs)\n",
    "        self.thresh = tf.constant(thresh, dtype=tf.float32)\n",
    "        self.nhood = neighborhood_size\n",
    "        self.use_local_avg = use_local_avg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.use_local_avg is True:\n",
    "          self.kernel_x = tf.reshape(tf.constant([[-1,0,1],[-1,0,1],[-1,0,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
    "          self.kernel_y = tf.reshape(tf.constant([[-1,-1,-1],[0,0,0],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
    "          self.kernel_sum = tf.reshape(tf.constant([[1,1,1],[1,1,1],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # local maxima positions\n",
    "        max_pool_image = MaxPooling2D(pool_size=(self.nhood,self.nhood), strides=(1,1), padding='same')(inputs)\n",
    "        cond = tf.math.greater(max_pool_image, self.thresh) & tf.math.equal(max_pool_image, inputs)\n",
    "        indices = tf.where(cond)\n",
    "        bind, xind, yind = indices[:, 0], indices[:, 2], indices[:, 1]\n",
    "        confidence = tf.gather_nd(inputs, indices)\n",
    "\n",
    "        # local CoG estimator\n",
    "        if self.use_local_avg:\n",
    "          x_image = K.conv2d(inputs, self.kernel_x, padding='same')\n",
    "          y_image = K.conv2d(inputs, self.kernel_y, padding='same')\n",
    "          sum_image = K.conv2d(inputs, self.kernel_sum, padding='same')\n",
    "          confidence = tf.cast(tf.gather_nd(sum_image, indices), dtype=tf.float32)\n",
    "          x_local = tf.math.divide(tf.gather_nd(x_image, indices),tf.gather_nd(sum_image, indices))\n",
    "          y_local = tf.math.divide(tf.gather_nd(y_image, indices),tf.gather_nd(sum_image, indices))\n",
    "          xind = tf.cast(xind, dtype=tf.float32) + tf.cast(x_local, dtype=tf.float32)\n",
    "          yind = tf.cast(yind, dtype=tf.float32) + tf.cast(y_local, dtype=tf.float32)\n",
    "        else:\n",
    "          xind = tf.cast(xind, dtype=tf.float32)\n",
    "          yind = tf.cast(yind, dtype=tf.float32)\n",
    "        \n",
    "        return bind, xind, yind, confidence\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        # Implement get_config to enable serialization. This is optional.\n",
    "        base_config = super(Maximafinder, self).get_config()\n",
    "        config = {}\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------- Prediction with postprocessing  function-------------------------------\n",
    "def batchFramePredictionLocalization(dataPath, filename, modelPath, savePath, batch_size=1, thresh=0.1, neighborhood_size=3, use_local_avg = False, pixel_size = None):\n",
    "    \"\"\"\n",
    "    This function tests a trained model on the desired test set, given the \n",
    "    tiff stack of test images, learned weights, and normalization factors.\n",
    "    \n",
    "    # Inputs\n",
    "    dataPath          - the path to the folder containing the tiff stack(s) to run prediction on \n",
    "    filename          - the name of the file to process\n",
    "    modelPath         - the path to the folder containing the weights file and the mean and standard deviation file generated in train_model\n",
    "    savePath          - the path to the folder where to save the prediction\n",
    "    batch_size.       - the number of frames to predict on for each iteration\n",
    "    thresh            - threshoold percentage from the maximum of the gaussian scaling\n",
    "    neighborhood_size - the size of the neighborhood for local maxima finding\n",
    "    use_local_average - Boolean whether to perform local averaging or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # load mean and std\n",
    "    matfile = sio.loadmat(os.path.join(modelPath,'model_metadata.mat'))\n",
    "    test_mean = np.array(matfile['mean_test'])\n",
    "    test_std = np.array(matfile['std_test'])  \n",
    "    upsampling_factor = np.array(matfile['upsampling_factor'])\n",
    "    upsampling_factor = upsampling_factor.item() # convert to scalar\n",
    "    L2_weighting_factor = np.array(matfile['Normalization factor'])\n",
    "    L2_weighting_factor = L2_weighting_factor.item() # convert to scalar\n",
    "\n",
    "    # Read in the raw file\n",
    "    Images = io.imread(os.path.join(dataPath, filename))\n",
    "    if pixel_size == None:\n",
    "      pixel_size, _, _ = getPixelSizeTIFFmetadata(os.path.join(dataPath, filename), display=True)\n",
    "    pixel_size_hr = pixel_size/upsampling_factor\n",
    "\n",
    "    # get dataset dimensions\n",
    "    (nFrames, M, N) = Images.shape\n",
    "    print('Input image is '+str(N)+'x'+str(M)+' with '+str(nFrames)+' frames.')\n",
    "\n",
    "    # Build the model for a bigger image\n",
    "    model = buildModel((upsampling_factor*M, upsampling_factor*N, 1))\n",
    "\n",
    "    # Load the trained weights\n",
    "    model.load_weights(os.path.join(modelPath,'weights_best.hdf5'))\n",
    "\n",
    "    # add a post-processing module\n",
    "    max_layer = Maximafinder(thresh*L2_weighting_factor, neighborhood_size, use_local_avg)\n",
    "\n",
    "    # Initialise the results: lists will be used to collect all the localizations\n",
    "    frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n",
    "\n",
    "    # Initialise the results\n",
    "    Prediction = np.zeros((M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n",
    "    Widefield = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "    # run model in batches\n",
    "    n_batches = math.ceil(nFrames/batch_size)\n",
    "    for b in tqdm(range(n_batches)):\n",
    "\n",
    "      nF = min(batch_size, nFrames - b*batch_size)\n",
    "      Images_norm = np.zeros((nF, M, N),dtype=np.float32)\n",
    "      Images_upsampled = np.zeros((nF, M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n",
    "\n",
    "      # Upsampling using a simple nearest neighbor interp and calculating - MULTI-THREAD this?\n",
    "      for f in range(nF):\n",
    "        Images_norm[f,:,:] = project_01(Images[b*batch_size+f,:,:])\n",
    "        Images_norm[f,:,:] = normalize_im(Images_norm[f,:,:], test_mean, test_std)\n",
    "        Images_upsampled[f,:,:] = np.kron(Images_norm[f,:,:], np.ones((upsampling_factor,upsampling_factor)))\n",
    "        Widefield += Images[b*batch_size+f,:,:]\n",
    "\n",
    "      # Reshaping\n",
    "      Images_upsampled = np.expand_dims(Images_upsampled,axis=3)\n",
    "\n",
    "      # Run prediction and local amxima finding\n",
    "      predicted_density = model.predict_on_batch(Images_upsampled)\n",
    "      predicted_density[predicted_density < 0] = 0\n",
    "      Prediction += predicted_density.sum(axis = 3).sum(axis = 0)\n",
    "\n",
    "      bind, xind, yind, confidence = max_layer(predicted_density)\n",
    "      \n",
    "      # normalizing the confidence by the L2_weighting_factor\n",
    "      confidence /= L2_weighting_factor \n",
    "\n",
    "      # turn indices to nms and append to the results\n",
    "      xind, yind = xind*pixel_size_hr, yind*pixel_size_hr\n",
    "      frmind = (bind.numpy() + b*batch_size + 1).tolist()\n",
    "      xind = xind.numpy().tolist()\n",
    "      yind = yind.numpy().tolist()\n",
    "      confidence = confidence.numpy().tolist()\n",
    "      frame_number_list += frmind\n",
    "      x_nm_list += xind\n",
    "      y_nm_list += yind\n",
    "      confidence_au_list += confidence\n",
    "\n",
    "    # Open and create the csv file that will contain all the localizations\n",
    "    if use_local_avg:\n",
    "      ext = '_avg'\n",
    "    else:\n",
    "      ext = '_max'\n",
    "    with open(os.path.join(savePath, 'Localizations_' + os.path.splitext(filename)[0] + ext + '.csv'), \"w\", newline='') as file:\n",
    "      writer = csv.writer(file)\n",
    "      writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n",
    "      locs = list(zip(frame_number_list, x_nm_list, y_nm_list, confidence_au_list))\n",
    "      writer.writerows(locs)\n",
    "\n",
    "    # Save the prediction and widefield image\n",
    "    Widefield = np.kron(Widefield, np.ones((upsampling_factor,upsampling_factor)))\n",
    "    Widefield = np.float32(Widefield)\n",
    "\n",
    "    # io.imsave(os.path.join(savePath, 'Predicted_'+os.path.splitext(filename)[0]+'.tif'), Prediction)\n",
    "    # io.imsave(os.path.join(savePath, 'Widefield_'+os.path.splitext(filename)[0]+'.tif'), Widefield)\n",
    "\n",
    "    saveAsTIF(savePath, 'Predicted_'+os.path.splitext(filename)[0], Prediction, pixel_size_hr)\n",
    "    saveAsTIF(savePath, 'Widefield_'+os.path.splitext(filename)[0], Widefield, pixel_size_hr)\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Colors for the warning messages\n",
    "class bcolors:\n",
    "  WARNING = '\\033[31m'\n",
    "  NORMAL = '\\033[0m'  # white (normal)\n",
    "\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "  return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "\n",
    "# @njit(parallel=True)\n",
    "def subPixelMaxLocalization(array, method = 'CoM', patch_size = 3):\n",
    "  xMaxInd, yMaxInd = np.unravel_index(array.argmax(), array.shape, order='C')\n",
    "  centralPatch = XC[(xMaxInd-patch_size):(xMaxInd+patch_size+1),(yMaxInd-patch_size):(yMaxInd+patch_size+1)]\n",
    "\n",
    "  if (method == 'MAX'):\n",
    "    x0 = xMaxInd\n",
    "    y0 = yMaxInd\n",
    "\n",
    "  elif (method == 'CoM'):\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    S = 0\n",
    "    for xy in range(patch_size*patch_size):\n",
    "      y = math.floor(xy/patch_size)\n",
    "      x = xy - y*patch_size\n",
    "      x0 += x*array[x,y]\n",
    "      y0 += y*array[x,y]\n",
    "      S = array[x,y]\n",
    "    \n",
    "    x0 = x0/S - patch_size/2 + xMaxInd\n",
    "    y0 = y0/S - patch_size/2 + yMaxInd\n",
    "  \n",
    "  elif (method == 'Radiality'):\n",
    "    # Not implemented yet\n",
    "    x0 = xMaxInd\n",
    "    y0 = yMaxInd\n",
    "  \n",
    "  return (x0, y0)\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n",
    "  n_locs = xc_array.shape[0]\n",
    "  xc_array_Corr = np.empty(n_locs)\n",
    "  yc_array_Corr = np.empty(n_locs)\n",
    "  \n",
    "  for loc in prange(n_locs):\n",
    "    xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc]]\n",
    "    yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc]]\n",
    "\n",
    "  return (xc_array_Corr, yc_array_Corr)\n",
    "\n",
    "\n",
    "print('--------------------------------')\n",
    "print('DeepSTORM installation complete.')\n",
    "\n",
    "# Check if this is the latest version of the notebook\n",
    "\n",
    "All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
    "print('Notebook version: '+Notebook_version)\n",
    "\n",
    "Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
    "print('Latest notebook version: '+Latest_Notebook_version)\n",
    "\n",
    "\n",
    "if Notebook_version == Latest_Notebook_version:\n",
    "  print(\"This notebook is up-to-date.\")\n",
    "else:\n",
    "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
    "\n",
    "\n",
    "# Latest_notebook_version = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_ZeroCostDL4Mic_Release.csv\")\n",
    "\n",
    "# if Notebook_version == list(Latest_notebook_version.columns):\n",
    "#   print(\"This notebook is up-to-date.\")\n",
    "\n",
    "# if not Notebook_version == list(Latest_notebook_version.columns):\n",
    "#   print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
    "\n",
    "def pdf_export(trained = False, raw_data = False, pretrained_model = False):\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B') \n",
    "\n",
    "  \n",
    "  #model_name = 'little_CARE_test'\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n",
    "  pdf.ln(1)\n",
    "    \n",
    "  # add another cell \n",
    "  if trained:\n",
    "    training_time = \"Training time: \"+str(hours)+ \"hour(s) \"+str(minutes)+\"min(s) \"+str(round(seconds))+\"sec(s)\"\n",
    "    pdf.cell(190, 5, txt = training_time, new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  Header_2 = 'Information for your materials and method:'\n",
    "  pdf.cell(190, 5, txt=Header_2,  new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='L')\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "  #print(all_packages)\n",
    "\n",
    "  #Main Packages\n",
    "  main_packages = ''\n",
    "  version_numbers = []\n",
    "  for name in ['tensorflow','numpy','keras']:\n",
    "    find_name=all_packages.find(name)\n",
    "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
    "    #Version numbers only here:\n",
    "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
    "\n",
    "  cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
    "  cuda_version = cuda_version.stdout.decode('utf-8')\n",
    "  cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
    "  gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
    "  gpu_name = gpu_name.stdout.decode('utf-8')\n",
    "  gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
    "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
    "  #print(gpu_name)\n",
    "  if raw_data == True:\n",
    "    shape = (M,N)\n",
    "  else:\n",
    "    shape = (int(FOV_size/pixel_size),int(FOV_size/pixel_size))\n",
    "  #dataset_size = len(os.listdir(Training_source))\n",
    "\n",
    "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(n_patches)+' paired image patches (image dimensions: '+str(patch_size)+', patch size (upsampled): ('+str(int(patch_size))+','+str(int(patch_size))+') with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Losses were calculated using MSE for the heatmaps and L1 loss for the spike prediction. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), keras (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+' GPU.'\n",
    "\n",
    "  if pretrained_model:\n",
    "    text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(n_patches)+' paired image patches (image dimensions: '+str(patch_size)+', patch size (upsampled): ('+str(int(patch_size))+','+str(int(patch_size))+') with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Losses were calculated using MSE for the heatmaps and L1 loss for the spike prediction. The models was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), keras (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+' GPU.'\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  pdf.multi_cell(180, 5, txt = text, align='L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "  pdf.cell(190, 5, txt = 'Training dataset', align='L',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  if raw_data==False:\n",
    "    simul_text = 'The training dataset was created in the notebook using the following simulation settings:'\n",
    "    pdf.cell(200, 5, txt=simul_text, align='L')\n",
    "    pdf.ln(1)\n",
    "    html = \"\"\" \n",
    "    <table width=60% style=\"margin-left:0px;\">\n",
    "      <tr>\n",
    "        <th width = 50% align=\"left\">Setting</th>\n",
    "        <th width = 50% align=\"left\">Simulated Value</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>FOV_size</td>\n",
    "        <td width = 50%>{0}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>pixel_size</td>\n",
    "        <td width = 50%>{1}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>ADC_per_photon_conversion</td>\n",
    "        <td width = 50%>{2}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>ReadOutNoise_ADC</td>\n",
    "        <td width = 50%>{3}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>ADC_offset</td>\n",
    "        <td width = 50%>{4}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>emitter_density</td>\n",
    "        <td width = 50%>{5}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>emitter_density_std</td>\n",
    "        <td width = 50%>{6}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>number_of_frames</td>\n",
    "        <td width = 50%>{7}</td>\n",
    "      </tr> \n",
    "      <tr>\n",
    "        <td width = 50%>sigma</td>\n",
    "        <td width = 50%>{8}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>sigma_std</td>\n",
    "        <td width = 50%>{9}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>n_photons</td>\n",
    "        <td width = 50%>{10}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>n_photons_std</td>\n",
    "        <td width = 50%>{11}</td>\n",
    "      </tr> \n",
    "    </table>\n",
    "    \"\"\".format(FOV_size, pixel_size, ADC_per_photon_conversion, ReadOutNoise_ADC, ADC_offset, emitter_density, emitter_density_std, number_of_frames, sigma, sigma_std, n_photons, n_photons_std)\n",
    "    pdf.write_html(html)\n",
    "  else:\n",
    "    simul_text = 'The training dataset was simulated using ThunderSTORM and loaded into the notebook.'\n",
    "    pdf.multi_cell(190, 5, txt=simul_text, align='L')\n",
    "    pdf.ln(1)\n",
    "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "    #pdf.ln(1)\n",
    "    #pdf.cell(190, 5, txt = 'Training Dataset', align='L',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size = 10, style = 'B')\n",
    "    pdf.cell(29, 5, txt= 'ImageData_path', align = 'L', new_x=XPos.RIGHT, new_y=YPos.TOP)\n",
    "    pdf.set_font('')\n",
    "    pdf.multi_cell(170, 5, txt = ImageData_path, align = 'L')\n",
    "    pdf.ln(1)\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size = 10, style = 'B')\n",
    "    pdf.cell(28, 5, txt= 'LocalizationData_path:', align = 'L', new_x=XPos.RIGHT, new_y=YPos.TOP)\n",
    "    pdf.set_font('')\n",
    "    pdf.multi_cell(170, 5, txt = LocalizationData_path, align = 'L')\n",
    "    pdf.ln(1)\n",
    "    pdf.set_font('Arial', size = 10, style = 'B')\n",
    "    pdf.cell(28, 5, txt= 'pixel_size:', align = 'L', new_x=XPos.RIGHT, new_y=YPos.TOP)\n",
    "    pdf.set_font('')\n",
    "    pdf.multi_cell(170, 5, txt = str(pixel_size), align = 'L')\n",
    "    pdf.ln(1)\n",
    "  #pdf.cell(190, 5, txt=aug_text, align='L',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Parameters', align='L',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  # if Use_Default_Advanced_Parameters:\n",
    "  #   pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
    "  pdf.cell(200, 5, txt='The following parameters were used to generate patches:')\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\" \n",
    "  <table width=70% style=\"margin-left:0px;\">\n",
    "    <tr>\n",
    "      <th width = 50% align=\"left\">Training Parameter</th>\n",
    "      <th width = 50% align=\"left\">Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>patch_size</td>\n",
    "      <td width = 50%>{0}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>upsampling_factor</td>\n",
    "      <td width = 50%>{1}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>num_patches_per_frame</td>\n",
    "      <td width = 50%>{2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>min_number_of_emitters_per_patch</td>\n",
    "      <td width = 50%>{3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>max_num_patches</td>\n",
    "      <td width = 50%>{4}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>gaussian_sigma</td>\n",
    "      <td width = 50%>{5}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>Automatic_normalization</td>\n",
    "      <td width = 50%>{6}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>L2_weighting_factor</td>\n",
    "      <td width = 50%>{7}</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  \"\"\".format(str(patch_size)+'x'+str(patch_size), upsampling_factor, num_patches_per_frame, min_number_of_emitters_per_patch, max_num_patches, gaussian_sigma, Automatic_normalization, L2_weighting_factor)\n",
    "  pdf.write_html(html)\n",
    "  pdf.ln(3)\n",
    "  pdf.set_font('Arial', size=10)\n",
    "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\" \n",
    "  <table width=70% style=\"margin-left:0px;\">\n",
    "    <tr>\n",
    "      <th width = 50% align=\"left\">Training Parameter</th>\n",
    "      <th width = 50% align=\"left\">Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>number_of_epochs</td>\n",
    "      <td width = 50%>{0}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>batch_size</td>\n",
    "      <td width = 50%>{1}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>number_of_steps</td>\n",
    "      <td width = 50%>{2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>percentage_validation</td>\n",
    "      <td width = 50%>{3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>initial_learning_rate</td>\n",
    "      <td width = 50%>{4}</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  \"\"\".format(number_of_epochs,batch_size,number_of_steps,percentage_validation,initial_learning_rate)\n",
    "  pdf.write_html(html)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  # pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(21, 5, txt= 'Model Path:', align = 'L', new_x=XPos.RIGHT, new_y=YPos.TOP)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(60, 5, txt = 'Example Training Images',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread('/content/TrainingDataExample_DeepSTORM2D.png').shape\n",
    "  pdf.image('/content/TrainingDataExample_DeepSTORM2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  pdf.ln(1)\n",
    "  ref_2 = '- Deep-STORM: Nehme, Elias, et al. \"Deep-STORM: super-resolution single-molecule microscopy by deep learning.\" Optica 5.4 (2018): 458-464.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  pdf.ln(1)\n",
    "  # if Use_Data_augmentation:\n",
    "  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
    "  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
    "  pdf.ln(3)\n",
    "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.output(model_path+'/'+model_name+'/'+model_name+'_training_report.pdf')\n",
    "  print('------------------------------')\n",
    "  print('PDF report exported in '+model_path+'/'+model_name+'/')\n",
    "\n",
    "def qc_pdf_export():\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B') \n",
    "\n",
    "  Network = 'Deep-STORM'\n",
    "  #model_name = os.path.basename(full_QC_model_path)\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Quality Control report for '+Network+' model ('+os.path.basename(QC_model_path)+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n",
    "  pdf.ln(1)\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(2)\n",
    "  pdf.cell(190, 5, txt = 'Loss curves',  new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='L')\n",
    "  pdf.ln(1)\n",
    "  if os.path.exists(savePath+'/lossCurvePlots.png'):\n",
    "    exp_size = io.imread(savePath+'/lossCurvePlots.png').shape\n",
    "    pdf.image(savePath+'/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/10))\n",
    "  else:\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size=10)\n",
    "    pdf.cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.')\n",
    "  pdf.ln(2)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.ln(3)\n",
    "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(savePath+'/QC_example_data.png').shape\n",
    "  pdf.image(savePath+'/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L',  new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\"\n",
    "  <body>\n",
    "  <font size=\"7\" face=\"Courier\" >\n",
    "  <table width=94% style=\"margin-left:0px;\">\"\"\"\n",
    "  with open(savePath+'/'+os.path.basename(QC_model_path)+'_QC_metrics.csv', 'r') as csvfile:\n",
    "    metrics = csv.reader(csvfile)\n",
    "    header = next(metrics)\n",
    "    image = header[0]\n",
    "    mSSIM_PvsGT = header[1]\n",
    "    mSSIM_SvsGT = header[2]\n",
    "    NRMSE_PvsGT = header[3]\n",
    "    NRMSE_SvsGT = header[4]\n",
    "    PSNR_PvsGT = header[5]\n",
    "    PSNR_SvsGT = header[6]\n",
    "    header = \"\"\"\n",
    "    <tr>\n",
    "    <th width = 10% align=\"left\">{0}</th>\n",
    "    <th width = 15% align=\"left\">{1}</th>\n",
    "    <th width = 15% align=\"center\">{2}</th>\n",
    "    <th width = 15% align=\"left\">{3}</th>\n",
    "    <th width = 15% align=\"center\">{4}</th>\n",
    "    <th width = 15% align=\"left\">{5}</th>\n",
    "    <th width = 15% align=\"center\">{6}</th>\n",
    "    </tr>\"\"\".format(image,mSSIM_PvsGT,mSSIM_SvsGT,NRMSE_PvsGT,NRMSE_SvsGT,PSNR_PvsGT,PSNR_SvsGT)\n",
    "    html = html+header\n",
    "    for row in metrics:\n",
    "      image = row[0]\n",
    "      mSSIM_PvsGT = row[1]\n",
    "      mSSIM_SvsGT = row[2]\n",
    "      NRMSE_PvsGT = row[3]\n",
    "      NRMSE_SvsGT = row[4]\n",
    "      PSNR_PvsGT = row[5]\n",
    "      PSNR_SvsGT = row[6]\n",
    "      cells = \"\"\"\n",
    "        <tr>\n",
    "          <td width = 10% align=\"left\">{0}</td>\n",
    "          <td width = 15% align=\"center\">{1}</td>\n",
    "          <td width = 15% align=\"center\">{2}</td>\n",
    "          <td width = 15% align=\"center\">{3}</td>\n",
    "          <td width = 15% align=\"center\">{4}</td>\n",
    "          <td width = 15% align=\"center\">{5}</td>\n",
    "          <td width = 15% align=\"center\">{6}</td>\n",
    "        </tr>\"\"\".format(image,str(round(float(mSSIM_PvsGT),3)),str(round(float(mSSIM_SvsGT),3)),str(round(float(NRMSE_PvsGT),3)),str(round(float(NRMSE_SvsGT),3)),str(round(float(PSNR_PvsGT),3)),str(round(float(PSNR_SvsGT),3)))\n",
    "      html = html+cells\n",
    "    html = html+\"\"\"</body></table>\"\"\"\n",
    "    \n",
    "  pdf.write_html(html)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  pdf.ln(1)\n",
    "  ref_2 = '- Deep-STORM: Nehme, Elias, et al. \"Deep-STORM: super-resolution single-molecule microscopy by deep learning.\" Optica 5.4 (2018): 458-464.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.ln(3)\n",
    "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
    "\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.output(savePath+'/'+os.path.basename(QC_model_path)+'_QC_report.pdf')\n",
    "\n",
    "\n",
    "  print('------------------------------')\n",
    "  print('QC PDF report exported as '+savePath+'/'+os.path.basename(QC_model_path)+'_QC_report.pdf')\n",
    "\n",
    "# Build requirements file for local run\n",
    "after = [str(m) for m in sys.modules]\n",
    "build_requirements_file(before, after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu8f5NGJkJos"
   },
   "source": [
    "\n",
    "# **3. Generate patches for training**\n",
    "---\n",
    "\n",
    "For Deep-STORM the training data can be obtained in two ways:\n",
    "* Simulated using ThunderSTORM or other simulation tool and loaded here (**using Section 3.1.a**)\n",
    "* Directly simulated in this notebook (**using Section 3.1.b**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSV8xnlynp0l"
   },
   "source": [
    "## **3.1.a Load training data**\n",
    "---\n",
    "\n",
    "Here you can load your simulated data along with its corresponding localization file.\n",
    "*   The `pixel_size` is defined in nanometer (nm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "CT6SNcfNg6j0",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images: 120x40 with 6000 frames\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4700aa7d314c8e8db13a5bcbc52156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='frame', max=6000, min=1), Outpu"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y [nm]</th>\n",
       "      <th>x [nm]</th>\n",
       "      <th>Photon #</th>\n",
       "      <th>frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92996</th>\n",
       "      <td>1750.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1415</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92997</th>\n",
       "      <td>3750.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>3596</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92998</th>\n",
       "      <td>1750.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>2846</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92999</th>\n",
       "      <td>4950.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2727</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93000</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4698</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y [nm]  x [nm]  Photon #  frame\n",
       "92996  1750.0   700.0      1415   6000\n",
       "92997  3750.0   700.0      3596   6000\n",
       "92998  1750.0  1050.0      2846   6000\n",
       "92999  4950.0  1100.0      2727   6000\n",
       "93000  1200.0  1000.0      4698   6000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown ##Load raw data\n",
    "\n",
    "load_raw_data = True\n",
    "\n",
    "# Get user input\n",
    "ImageData_path = \"DEEPSTORM_training_data_infinite_SNR/SimulatedDataset.tif\" #@param {type:\"string\"}\n",
    "LocalizationData_path = \"DEEPSTORM_training_data_infinite_SNR/SimulatedDataset.csv\" #@param {type: \"string\"}\n",
    "#@markdown Get pixel size from file?\n",
    "get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, use this value:\n",
    "pixel_size = 50 #@param {type:\"number\"}\n",
    "\n",
    "if get_pixel_size_from_file:\n",
    "  pixel_size,_,_ = getPixelSizeTIFFmetadata(ImageData_path, True)\n",
    "\n",
    "# load the tiff data\n",
    "Images = io.imread(ImageData_path)\n",
    "# get dataset dimensions\n",
    "if len(Images.shape) == 3:\n",
    "  (number_of_frames, M, N) = Images.shape\n",
    "elif len(Images.shape) == 2:\n",
    "  (M, N) = Images.shape\n",
    "  number_of_frames = 1\n",
    "print('Loaded images: '+str(M)+'x'+str(N)+' with '+str(number_of_frames)+' frames')\n",
    "\n",
    "# Interactive display of the stack\n",
    "def scroll_in_time(frame):\n",
    "    f=plt.figure(figsize=(6,6))\n",
    "    plt.imshow(Images[frame-1], interpolation='nearest', cmap = 'gray')\n",
    "    plt.title('Training source at frame = ' + str(frame))\n",
    "    plt.axis('off');\n",
    "\n",
    "if number_of_frames > 1:\n",
    "  interact(scroll_in_time, frame=widgets.IntSlider(min=1, max=Images.shape[0], step=1, value=0, continuous_update=False));\n",
    "else:\n",
    "  f=plt.figure(figsize=(6,6))\n",
    "  plt.imshow(Images, interpolation='nearest', cmap = 'gray')\n",
    "  plt.title('Training source')\n",
    "  plt.axis('off');\n",
    "\n",
    "# Load the localization file and display the first\n",
    "LocData = pd.read_csv(LocalizationData_path, index_col=0)\n",
    "LocData.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_8e3kE-JhVY"
   },
   "source": [
    "## **3.2. Generate training patches**\n",
    "---\n",
    "\n",
    "Training patches need to be created from the training data generated above. \n",
    "*   The `patch_size` needs to give sufficient contextual information and for most cases a `patch_size` of 26 (corresponding to patches of 26x26 pixels) works fine. **DEFAULT: 26**\n",
    "*   The `upsampling_factor` defines the effective magnification of the final super-resolved image compared to the input image (this is called magnification in ThunderSTORM). This is used to generate the super-resolved patches as target dataset. Using an `upsampling_factor` of 16 will require the use of more memory and it may be necessary to decreae the `patch_size` to 16 for example. **DEFAULT: 8**\n",
    "*   The `num_patches_per_frame` defines the number of patches extracted from each frame generated in section 3.1. **DEFAULT: 500**\n",
    "*   The `min_number_of_emitters_per_patch` defines the minimum number of emitters that need to be present in the patch to be a valid patch. An empty patch does not contain useful information for the network to learn from. **DEFAULT: 7**\n",
    "*   The `max_num_patches` defines the maximum number of patches to generate. Fewer may be generated depending on how many pacthes are rejected and how many frames are available. **DEFAULT: 10000**\n",
    "*   The `gaussian_sigma` defines the Gaussian standard deviation (in magnified pixels) applied to generate the super-resolved target image. **DEFAULT: 1**\n",
    "*   The `L2_weighting_factor` is a normalization factor used in the loss function. It helps balancing the loss from the L2 norm. When using higher densities, this factor should be decreased and vice-versa. This factor can be autimatically calculated using an empiraical formula. **DEFAULT: 100**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612,
     "referenced_widgets": [
      "276718597a1e437d9306f056154efe42",
      "9f03eafa3bb0459bb6bed9c1f0d2b855",
      "082403d267e14b4988a1835f64a16a5f",
      "3b6433ecff6b4b3ca4c39209acb78933",
      "5cd5905f042a4e08a0e10757487db630",
      "468ee561c8ee43d896a9f06578ca43f8",
      "4e01321bb3bc4114a51248efc09e8d64"
     ]
    },
    "id": "AsNx5KzcFNvC",
    "outputId": "2d932a8f-fd7b-457e-850c-ef2ce51975f2",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 patches of 104x104\n",
      "Total number of localizations: 93000\n",
      "Density: 1.29 locs/um^2\n",
      "Normalization factor: 100.7\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6000/6000 [00:12<00:00, 484.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches skipped due to low density: 1135833\n",
      "50000 patches were generated.\n",
      "Time elapsed: 0.0 hour(s) 0.0 min(s) 12 sec(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8dce60a0454e94a216b34cb7cc3129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='patch', max=50000, min=1), Outp"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown ## **Provide patch parameters**\n",
    "\n",
    "\n",
    "# -------------------- User input --------------------\n",
    "patch_size = 26 #@param {type:\"integer\"}\n",
    "upsampling_factor = 4 #@param [\"4\", \"8\", \"16\"] {type:\"raw\"}\n",
    "num_patches_per_frame =  500#@param {type:\"integer\"}\n",
    "min_number_of_emitters_per_patch = 5#@param {type:\"integer\"}\n",
    "max_num_patches =  50000#@param {type:\"integer\"}\n",
    "gaussian_sigma = 1#@param {type:\"integer\"}\n",
    "\n",
    "#@markdown Estimate the optimal normalization factor automatically?\n",
    "Automatic_normalization = True #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, it will use the following value:\n",
    "L2_weighting_factor = 100 #@param {type:\"number\"}\n",
    "\n",
    "\n",
    "# -------------------- Prepare variables --------------------\n",
    "# Start the clock to measure how long it takes\n",
    "start = time.time()\n",
    "\n",
    "# Initialize some parameters\n",
    "pixel_size_hr = pixel_size/upsampling_factor # in nm\n",
    "n_patches = min(number_of_frames*num_patches_per_frame, max_num_patches)\n",
    "patch_size = patch_size*upsampling_factor\n",
    "\n",
    "# Dimensions of the high-res grid\n",
    "Mhr = upsampling_factor*M # in pixels\n",
    "Nhr = upsampling_factor*N # in pixels\n",
    "\n",
    "# Initialize the training patches and labels\n",
    "patches = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n",
    "spikes = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n",
    "heatmaps = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n",
    "\n",
    "# Run over all frames and construct the training examples\n",
    "k = 1 # current patch count\n",
    "skip_counter = 0 # number of dataset skipped due to low density\n",
    "id_start = 0 # id position in LocData for current frame\n",
    "print('Generating '+str(n_patches)+' patches of '+str(patch_size)+'x'+str(patch_size))\n",
    "\n",
    "n_locs = len(LocData.index)\n",
    "print('Total number of localizations: '+str(n_locs))\n",
    "density = n_locs/(M*N*number_of_frames*(0.001*pixel_size)**2)\n",
    "print('Density: '+str(round(density,2))+' locs/um^2')\n",
    "n_locs_per_patch = patch_size**2*density\n",
    "\n",
    "if Automatic_normalization:\n",
    "  # This empirical formulae attempts to balance the loss L2 function between the background and the bright spikes\n",
    "  # A value of 100 was originally chosen to balance L2 for a patch size of 2.6x2.6^2 0.1um pixel size and density of 3 (hence the 20.28), at upsampling_factor = 8\n",
    "  L2_weighting_factor = 100/math.sqrt(min(n_locs_per_patch, min_number_of_emitters_per_patch)*8**2/(upsampling_factor**2*20.28))\n",
    "  print('Normalization factor: '+str(round(L2_weighting_factor,2)))\n",
    "\n",
    "# -------------------- Patch generation loop --------------------\n",
    "\n",
    "print('-----------------------------------------------------------')\n",
    "for (f, thisFrame) in enumerate(tqdm(Images)):\n",
    "\n",
    "  # Upsample the frame\n",
    "  upsampledFrame = np.kron(thisFrame, np.ones((upsampling_factor,upsampling_factor)))\n",
    "  # Read all the provided high-resolution locations for current frame\n",
    "  DataFrame = LocData[LocData['frame'] == f+1].copy()\n",
    "\n",
    "  # Get the approximated locations according to the high-res grid pixel size\n",
    "  Chr_emitters = [int(max(min(round(DataFrame['x [nm]'][i]/pixel_size_hr),Nhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n",
    "  Rhr_emitters = [int(max(min(round(DataFrame['y [nm]'][i]/pixel_size_hr),Mhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n",
    "  id_start += len(DataFrame.index)\n",
    "\n",
    "  # Build Localization image\n",
    "  LocImage = np.zeros((Mhr,Nhr))\n",
    "  LocImage[(Rhr_emitters, Chr_emitters)] = 1\n",
    "\n",
    "  # Here, there's a choice between the original Gaussian (classification approach) and using the erf function\n",
    "  HeatMapImage = L2_weighting_factor*gaussian_filter(LocImage, gaussian_sigma)  \n",
    "  # HeatMapImage = L2_weighting_factor*FromLoc2Image_MultiThreaded(np.array(list(DataFrame['x [nm]'])), np.array(list(DataFrame['y [nm]'])), \n",
    "                                                            #  np.ones(len(DataFrame.index)), pixel_size_hr*gaussian_sigma*np.ones(len(DataFrame.index)), \n",
    "                                                            #  Mhr, pixel_size_hr)\n",
    "  \n",
    "\n",
    "  # Generate random position for the top left corner of the patch\n",
    "  xc = np.random.randint(0, Mhr-patch_size, size=num_patches_per_frame)\n",
    "  yc = np.random.randint(0, Nhr-patch_size, size=num_patches_per_frame)\n",
    "\n",
    "  for c in range(len(xc)):\n",
    "    if LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size].sum() < min_number_of_emitters_per_patch:\n",
    "      skip_counter += 1\n",
    "      continue\n",
    "    \n",
    "    else:\n",
    "        # Limit maximal number of training examples to 15k\n",
    "      if k > max_num_patches:\n",
    "        break\n",
    "      else:\n",
    "        # Assign the patches to the right part of the images\n",
    "        patches[k-1] = upsampledFrame[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n",
    "        spikes[k-1] = LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n",
    "        heatmaps[k-1] = HeatMapImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n",
    "        k += 1 # increment current patch count\n",
    "\n",
    "# Remove the empty data\n",
    "patches = patches[:k-1]\n",
    "spikes = spikes[:k-1]\n",
    "heatmaps = heatmaps[:k-1]\n",
    "n_patches = k-1\n",
    "\n",
    "# -------------------- Failsafe --------------------\n",
    "# Check if the size of the training set is smaller than 5k to notify user to simulate more images using ThunderSTORM\n",
    "if ((k-1) < 5000):\n",
    "  # W  = '\\033[0m'  # white (normal)\n",
    "  # R  = '\\033[31m' # red\n",
    "  print(bcolors.WARNING+'!! WARNING: Training set size is below 5K - Consider simulating more images in ThunderSTORM. !!'+bcolors.NORMAL)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Displays --------------------\n",
    "print('Number of patches skipped due to low density: '+str(skip_counter))\n",
    "# dataSize = int((getsizeof(patches)+getsizeof(heatmaps)+getsizeof(spikes))/(1024*1024)) #rounded in MB\n",
    "# print('Size of patches: '+str(dataSize)+' MB')\n",
    "print(str(n_patches)+' patches were generated.')\n",
    "\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "# Display patches interactively with a slider\n",
    "def scroll_patches(patch):\n",
    "  f = plt.figure(figsize=(16,6))\n",
    "  plt.subplot(1,3,1)\n",
    "  plt.imshow(patches[patch-1], interpolation='nearest', cmap='gray')\n",
    "  plt.title('Raw data (frame #'+str(patch)+')')\n",
    "  plt.axis('off');\n",
    "\n",
    "  plt.subplot(1,3,2)\n",
    "  plt.imshow(heatmaps[patch-1], interpolation='nearest')\n",
    "  plt.title('Heat map')\n",
    "  plt.axis('off');\n",
    "\n",
    "  plt.subplot(1,3,3)\n",
    "  plt.imshow(spikes[patch-1], interpolation='nearest')\n",
    "  plt.title('Localization map')\n",
    "  plt.axis('off');\n",
    "  \n",
    "\n",
    "interact(scroll_patches, patch=widgets.IntSlider(min=1, max=patches.shape[0], step=1, value=0, continuous_update=False));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSjXFMevK7Iz"
   },
   "source": [
    "# **4. Train the network**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVeyKU0MdAPx"
   },
   "source": [
    "## **4.1. Select your paths and parameters**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n",
    "\n",
    "\n",
    "<font size = 5>**Training parameters**\n",
    "\n",
    "<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10-30) epochs, but a full training should run for ~100 epochs. Evaluate the performance after training (see 5). **Default value: 80**\n",
    "\n",
    "<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 16**\n",
    "\n",
    "<font size = 4>**`number_of_steps`:** Define the number of training steps by epoch. **If this value is set to 0**, by default this parameter is calculated so that each patch is seen at least once per epoch. **Default value: Number of patch / batch_size**\n",
    "\n",
    "<font size = 4>**`percentage_validation`:**  Input the percentage of your training dataset you want to use to validate the network during training. **Default value: 30** \n",
    "\n",
    "<font size = 4>**`initial_learning_rate`:** This parameter represents the initial value to be used as learning rate in the optimizer. **Default value: 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oa5cDZ7f_PF6",
    "outputId": "b3ef08c7-5af5-4e42-837e-8ed636d6e57a",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps: 2187\n",
      "\u001b[31mThe model folder already exists and will be overwritten.\u001b[0m\n",
      "-----------------------------\n",
      "Training parameters set.\n"
     ]
    }
   ],
   "source": [
    "#@markdown ###Path to training images and parameters\n",
    "\n",
    "model_path = \"models_infinite_SNR\" #@param {type: \"string\"} \n",
    "model_name = \"test\" #@param {type: \"string\"} \n",
    "number_of_epochs =  80#@param {type:\"integer\"}\n",
    "batch_size =  16#@param {type:\"integer\"}\n",
    "\n",
    "number_of_steps =  0#@param {type:\"integer\"}\n",
    "percentage_validation = 30 #@param {type:\"number\"}\n",
    "initial_learning_rate = 0.001 #@param {type:\"number\"}\n",
    "\n",
    "\n",
    "percentage_validation /= 100\n",
    "if number_of_steps == 0: \n",
    "  number_of_steps = int((1-percentage_validation)*n_patches/batch_size)\n",
    "  print('Number of steps: '+str(number_of_steps))\n",
    "\n",
    "# Pretrained model path initialised here so next cell does not need to be run\n",
    "h5_file_path = ''\n",
    "Use_pretrained_model = False\n",
    "\n",
    "if not ('patches' in locals()):\n",
    "  # W  = '\\033[0m'  # white (normal)\n",
    "  # R  = '\\033[31m' # red\n",
    "  print(WARNING+'!! WARNING: No patches were found in memory currently. !!')\n",
    "\n",
    "Save_path = os.path.join(model_path, model_name)\n",
    "if os.path.exists(Save_path):\n",
    "  print(bcolors.WARNING+'The model folder already exists and will be overwritten.'+bcolors.NORMAL)\n",
    "\n",
    "print('-----------------------------')\n",
    "print('Training parameters set.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIyEvQBWLp9n"
   },
   "source": [
    "\n",
    "## **4.2. Using weights from a pre-trained model as initial weights**\n",
    "---\n",
    "<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a Deep-STORM 2D model**. \n",
    "\n",
    "<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n",
    "\n",
    "<font size = 4> In order to continue training from the point where the pre-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "oHL5g0w8LqR0",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained network will be used.\n"
     ]
    }
   ],
   "source": [
    "# @markdown ##Loading weights from a pre-trained network\n",
    "\n",
    "Use_pretrained_model = False #@param {type:\"boolean\"}\n",
    "pretrained_model_choice = \"Model_from_file\" #@param [\"Model_from_file\"]\n",
    "Weights_choice = \"best\" #@param [\"last\", \"best\"]\n",
    "\n",
    "#@markdown ###If you chose \"Model_from_file\", please provide the path to the model folder:\n",
    "pretrained_model_path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# --------------------- Check if we load a previously trained model ------------------------\n",
    "if Use_pretrained_model:\n",
    "\n",
    "# --------------------- Load the model from the choosen path ------------------------\n",
    "  if pretrained_model_choice == \"Model_from_file\":\n",
    "    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n",
    "\n",
    "# --------------------- Download the a model provided in the XXX ------------------------\n",
    "\n",
    "  if pretrained_model_choice == \"Model_name\":\n",
    "    pretrained_model_name = \"Model_name\"\n",
    "    pretrained_model_path = \"/content/\"+pretrained_model_name\n",
    "    print(\"Downloading the 2D_Demo_Model_from_Stardist_2D_paper\")\n",
    "    if os.path.exists(pretrained_model_path):\n",
    "      shutil.rmtree(pretrained_model_path)\n",
    "    os.makedirs(pretrained_model_path)\n",
    "    wget.download(\"\", pretrained_model_path)\n",
    "    wget.download(\"\", pretrained_model_path)\n",
    "    wget.download(\"\", pretrained_model_path)    \n",
    "    wget.download(\"\", pretrained_model_path)\n",
    "    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n",
    "\n",
    "# --------------------- Add additional pre-trained models here ------------------------\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Check the model exist ------------------------\n",
    "# If the model path chosen does not contain a pretrain model then use_pretrained_model is disabled, \n",
    "  if not os.path.exists(h5_file_path):\n",
    "    print(bcolors.WARNING+'WARNING: weights_'+Weights_choice+'.hdf5 pretrained model does not exist'+bcolors.NORMAL)\n",
    "    Use_pretrained_model = False\n",
    "\n",
    "  \n",
    "# If the model path contains a pretrain model, we load the training rate, \n",
    "  if os.path.exists(h5_file_path):\n",
    "#Here we check if the learning rate can be loaded from the quality control folder\n",
    "    if os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n",
    "      with open(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv'),'r') as csvfile:\n",
    "        csvRead = pd.read_csv(csvfile, sep=',')\n",
    "        #print(csvRead)\n",
    "        if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n",
    "          print(\"pretrained network learning rate found\")\n",
    "          #find the last learning rate\n",
    "          lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n",
    "          #Find the learning rate corresponding to the lowest validation loss\n",
    "          min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n",
    "          #print(min_val_loss)\n",
    "          bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n",
    "          if Weights_choice == \"last\":\n",
    "            print('Last learning rate: '+str(lastLearningRate))\n",
    "          if Weights_choice == \"best\":\n",
    "            print('Learning rate of best validation loss: '+str(bestLearningRate))\n",
    "        if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n",
    "          bestLearningRate = initial_learning_rate\n",
    "          lastLearningRate = initial_learning_rate\n",
    "          print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead.'+bcolors.NORMAL)\n",
    "\n",
    "#Compatibility with models trained outside ZeroCostDL4Mic but default learning rate will be used\n",
    "    if not os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n",
    "      print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(initial_learning_rate)+' will be used instead'+bcolors.NORMAL)\n",
    "      bestLearningRate = initial_learning_rate\n",
    "      lastLearningRate = initial_learning_rate\n",
    "\n",
    "\n",
    "# Display info about the pretrained model to be loaded (or not)\n",
    "if Use_pretrained_model:\n",
    "  print('Weights found in:')\n",
    "  print(h5_file_path)\n",
    "  print('will be loaded prior to training.')\n",
    "\n",
    "else:\n",
    "  print('No pretrained network will be used.')\n",
    "  h5_file_path = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OADNcie-LHxA"
   },
   "source": [
    "## **4.4. Start Training**\n",
    "---\n",
    "<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n",
    "\n",
    "<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches.\n",
    "\n",
    "<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder from Google Drive as all data can be erased at the next training if using the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDgMu_mAK8US",
    "outputId": "fd2795d7-0b54-4c2b-d40a-836b08333e9e",
    "scrolled": true,
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 35000\n",
      "Number of validation examples: 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 104, 104, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF1 (Conv2D)          (None, 104, 104, 32)      288       \n",
      "_________________________________________________________________\n",
      "BN-CNNF1 (BatchNormalization (None, 104, 104, 32)      128       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF1 (Activation)      (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "CNNPool1 (MaxPooling2D)      (None, 52, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF2 (Conv2D)          (None, 52, 52, 64)        18432     \n",
      "_________________________________________________________________\n",
      "BN-CNNF2 (BatchNormalization (None, 52, 52, 64)        256       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF2 (Activation)      (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "CNNPool2 (MaxPooling2D)      (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF3 (Conv2D)          (None, 26, 26, 128)       73728     \n",
      "_________________________________________________________________\n",
      "BN-CNNF3 (BatchNormalization (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF3 (Activation)      (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "CNNPool3 (MaxPooling2D)      (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF4 (Conv2D)          (None, 13, 13, 512)       589824    \n",
      "_________________________________________________________________\n",
      "BN-CNNF4 (BatchNormalization (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "Relu-CNNF4 (Activation)      (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "CNNUpsample1 (UpSampling2D)  (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF5 (Conv2D)          (None, 26, 26, 128)       589824    \n",
      "_________________________________________________________________\n",
      "BN-CNNF5 (BatchNormalization (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF5 (Activation)      (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "CNNUpsample2 (UpSampling2D)  (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF6 (Conv2D)          (None, 52, 52, 64)        73728     \n",
      "_________________________________________________________________\n",
      "BN-CNNF6 (BatchNormalization (None, 52, 52, 64)        256       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF6 (Activation)      (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "CNNUpsample3 (UpSampling2D)  (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF7 (Conv2D)          (None, 104, 104, 32)      18432     \n",
      "_________________________________________________________________\n",
      "BN-CNNF7 (BatchNormalization (None, 104, 104, 32)      128       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF7 (Activation)      (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "Prediction (Conv2D)          (None, 104, 104, 1)       32        \n",
      "=================================================================\n",
      "Total params: 1,368,128\n",
      "Trainable params: 1,366,208\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Using random initial model weights.\n",
      "-------------------------------\n",
      "Training model...\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2024-04-20 15:45:09.027328: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-04-20 15:45:09.027589: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2995200000 Hz\n",
      "2024-04-20 15:45:09.332422: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-04-20 15:45:09.402583: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8907\n",
      "2024-04-20 15:45:09.512666: E tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-20 15:45:09.512691: W tensorflow/stream_executor/gpu/asm_compiler.cc:56] Couldn't invoke ptxas --version\n",
      "2024-04-20 15:45:09.513282: E tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-20 15:45:09.513332: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-04-20 15:45:09.593328: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-04-20 15:45:09.619610: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2187/2187 [==============================] - 13s 5ms/step - loss: 0.2226 - val_loss: 0.1626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16259, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 2/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.1383 - val_loss: 0.1287\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16259 to 0.12867, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 3/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.1144 - val_loss: 0.1124\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12867 to 0.11237, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 4/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0972 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11237 to 0.09743, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 5/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0855 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09743 to 0.08419, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 6/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0775 - val_loss: 0.0770\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08419 to 0.07699, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 7/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0714 - val_loss: 0.0721\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07699 to 0.07214, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 8/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0673 - val_loss: 0.0670\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07214 to 0.06697, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 9/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0639 - val_loss: 0.0715\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06697\n",
      "Epoch 10/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0614 - val_loss: 0.0653\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06697 to 0.06527, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 11/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0589 - val_loss: 0.0619\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06527 to 0.06190, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 12/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0571 - val_loss: 0.0609\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06190 to 0.06090, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 13/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0554 - val_loss: 0.0580\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06090 to 0.05805, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 14/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0540 - val_loss: 0.0571\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05805 to 0.05711, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 15/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0527 - val_loss: 0.0572\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.05711\n",
      "Epoch 16/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0519 - val_loss: 0.0558\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05711 to 0.05579, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 17/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0507 - val_loss: 0.0540\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05579 to 0.05400, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 18/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0499 - val_loss: 0.0541\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05400\n",
      "Epoch 19/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0491 - val_loss: 0.0540\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.05400\n",
      "Epoch 20/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0486 - val_loss: 0.0511\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05400 to 0.05110, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 21/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0479 - val_loss: 0.0524\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05110\n",
      "Epoch 22/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0475 - val_loss: 0.0520\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05110\n",
      "Epoch 23/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0468 - val_loss: 0.0505\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05110 to 0.05047, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 24/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0470 - val_loss: 0.0503\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05047 to 0.05034, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 25/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0461 - val_loss: 0.0513\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05034\n",
      "Epoch 26/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0457 - val_loss: 0.0488\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05034 to 0.04880, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 27/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0456 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04880\n",
      "Epoch 28/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0451 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.04880\n",
      "Epoch 29/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0447 - val_loss: 0.0492\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04880\n",
      "Epoch 30/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0445 - val_loss: 0.0476\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04880 to 0.04763, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 31/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0440 - val_loss: 0.0475\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04763 to 0.04754, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 32/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0437 - val_loss: 0.0462\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04754 to 0.04617, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 33/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0438 - val_loss: 0.0461\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04617 to 0.04613, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 34/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0433 - val_loss: 0.0459\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04613 to 0.04589, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 35/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0434 - val_loss: 0.0474\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.04589\n",
      "Epoch 36/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0456 - val_loss: 0.0457\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04589 to 0.04574, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 37/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0423 - val_loss: 0.0468\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.04574\n",
      "Epoch 38/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0430 - val_loss: 0.0460\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04574\n",
      "Epoch 39/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0427 - val_loss: 0.0460\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.04574\n",
      "Epoch 40/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0426 - val_loss: 0.0460\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.04574\n",
      "Epoch 41/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0422 - val_loss: 0.0457\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04574 to 0.04574, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 42/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0410 - val_loss: 0.0434\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04574 to 0.04343, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 43/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0407 - val_loss: 0.0432\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04343 to 0.04323, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 44/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0406 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04323 to 0.04315, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 45/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0405 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.04315 to 0.04307, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 46/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0405 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.04307 to 0.04304, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 47/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0405 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.04304 to 0.04300, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 48/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0405 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.04300 to 0.04296, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 49/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0404 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.04296 to 0.04289, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 50/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0404 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04289\n",
      "Epoch 51/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0404 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04289\n",
      "Epoch 52/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0404 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.04289 to 0.04283, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 53/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0404 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.04283\n",
      "Epoch 54/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0404 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.04283\n",
      "Epoch 55/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0403 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.04283\n",
      "Epoch 56/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0403 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.04283 to 0.04283, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 57/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0403 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.04283\n",
      "Epoch 58/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0403 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.04283 to 0.04279, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 59/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0403 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.04279 to 0.04275, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 60/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.04275 to 0.04274, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 61/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.04274\n",
      "Epoch 62/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.04274\n",
      "Epoch 63/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.04274\n",
      "Epoch 64/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.04274\n",
      "Epoch 65/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.04274 to 0.04273, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 66/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.04273 to 0.04272, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 67/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.04272\n",
      "Epoch 68/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.04272 to 0.04271, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 69/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.04271\n",
      "Epoch 70/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0402 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.04271 to 0.04270, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 71/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0401 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.04270\n",
      "Epoch 72/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0400 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.04270 to 0.04261, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 73/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0400 - val_loss: 0.0427\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.04261\n",
      "Epoch 74/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0400 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.04261 to 0.04258, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 75/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0400 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.04258 to 0.04258, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 76/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0400 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.04258 to 0.04257, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 77/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0400 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.04257 to 0.04253, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 78/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.04253\n",
      "Epoch 79/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.04253\n",
      "Epoch 80/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.04253\n",
      "Epoch 81/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.04253\n",
      "Epoch 82/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.04253\n",
      "Epoch 83/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.04253\n",
      "Epoch 84/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.04253 to 0.04253, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 85/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.04253\n",
      "Epoch 86/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.04253\n",
      "Epoch 87/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.04253\n",
      "Epoch 88/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.04253\n",
      "Epoch 89/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.04253\n",
      "Epoch 90/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.04253 to 0.04249, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 91/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.04249\n",
      "Epoch 92/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.04249 to 0.04249, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 93/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.04249\n",
      "Epoch 94/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.04249\n",
      "Epoch 95/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.04249\n",
      "Epoch 96/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0399 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.04249\n",
      "Epoch 97/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0398 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.04249\n",
      "Epoch 98/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0398 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.04249 to 0.04248, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 99/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0398 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.04248\n",
      "Epoch 100/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0398 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04248\n",
      "Epoch 101/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0397 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.04248\n",
      "Epoch 102/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0397 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.04248 to 0.04242, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 103/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0397 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.04242\n",
      "Epoch 104/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0397 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.04242\n",
      "Epoch 105/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0397 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.04242\n",
      "Epoch 106/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0397 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.04242\n",
      "Epoch 107/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.04242\n",
      "Epoch 108/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.04242 to 0.04240, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 109/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.04240 to 0.04238, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 110/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.04238\n",
      "Epoch 111/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0424\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.04238 to 0.04237, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 112/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.04237 to 0.04234, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 113/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.04234 to 0.04231, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 114/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0396 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.04231\n",
      "Epoch 115/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0395 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.04231 to 0.04231, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 116/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0395 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.04231\n",
      "Epoch 117/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0395 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.04231 to 0.04231, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 118/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0394 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.04231 to 0.04228, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 119/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0394 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.04228 to 0.04225, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 120/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0394 - val_loss: 0.0422\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.04225 to 0.04219, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 121/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0393 - val_loss: 0.0421\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.04219 to 0.04214, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 122/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0393 - val_loss: 0.0422\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.04214\n",
      "Epoch 123/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0393 - val_loss: 0.0422\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.04214\n",
      "Epoch 124/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0392 - val_loss: 0.0421\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.04214 to 0.04210, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 125/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0392 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.04210 to 0.04204, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 126/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0392 - val_loss: 0.0421\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.04204\n",
      "Epoch 127/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0392 - val_loss: 0.0421\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.04204\n",
      "Epoch 128/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0391 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.04204 to 0.04203, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 129/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0391 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.04203\n",
      "Epoch 130/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0391 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.04203 to 0.04199, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 131/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0391 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.04199\n",
      "Epoch 132/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0391 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.04199\n",
      "Epoch 133/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0391 - val_loss: 0.0419\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.04199 to 0.04192, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 134/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0390 - val_loss: 0.0419\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.04192 to 0.04190, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 135/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0390 - val_loss: 0.0419\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.04190 to 0.04189, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 136/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0390 - val_loss: 0.0418\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.04189 to 0.04185, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 137/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0390 - val_loss: 0.0419\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.04185\n",
      "Epoch 138/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0390 - val_loss: 0.0418\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.04185 to 0.04181, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 139/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0389 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.04181 to 0.04170, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 140/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0389 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.04170 to 0.04167, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 141/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0388 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.04167\n",
      "Epoch 142/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0388 - val_loss: 0.0416\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.04167 to 0.04162, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 143/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0388 - val_loss: 0.0416\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.04162 to 0.04158, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 144/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0387 - val_loss: 0.0416\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.04158 to 0.04157, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 145/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0387 - val_loss: 0.0416\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.04157\n",
      "Epoch 146/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0387 - val_loss: 0.0415\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.04157 to 0.04151, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 147/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0386 - val_loss: 0.0415\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.04151 to 0.04150, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 148/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0386 - val_loss: 0.0415\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.04150 to 0.04145, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 149/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0386 - val_loss: 0.0414\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.04145 to 0.04143, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "Epoch 150/150\n",
      "2187/2187 [==============================] - 11s 5ms/step - loss: 0.0385 - val_loss: 0.0414\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.04143 to 0.04139, saving model to models_infinite_SNR/test/weights_best.hdf5\n",
      "-------------------------------\n",
      "Training Complete!\n",
      "Time elapsed: 0.0 hour(s) 28.0 min(s) 8 sec(s)\n"
     ]
    }
   ],
   "source": [
    "#@markdown ##Start training\n",
    "\n",
    "# Start the clock to measure how long it takes\n",
    "start = time.time()\n",
    "\n",
    "# --------------------- Using pretrained model ------------------------\n",
    "#Here we ensure that the learning rate set correctly when using pre-trained models\n",
    "if Use_pretrained_model:\n",
    "  if Weights_choice == \"last\":\n",
    "    initial_learning_rate = lastLearningRate\n",
    "\n",
    "  if Weights_choice == \"best\":            \n",
    "    initial_learning_rate = bestLearningRate\n",
    "# --------------------- ---------------------- ------------------------\n",
    "\n",
    "\n",
    "#here we check that no model with the same name already exist, if so delete\n",
    "if os.path.exists(Save_path):\n",
    "  shutil.rmtree(Save_path)\n",
    "\n",
    "# Create the model folder!\n",
    "os.makedirs(Save_path)\n",
    "\n",
    "# Export pdf summary \n",
    "#pdf_export(raw_data = load_raw_data, pretrained_model = Use_pretrained_model)\n",
    "\n",
    "# Let's go !\n",
    "train_model(patches, heatmaps, Save_path, \n",
    "            steps_per_epoch=number_of_steps, epochs=150, batch_size=batch_size,\n",
    "            upsampling_factor = upsampling_factor,\n",
    "            validation_split = percentage_validation,\n",
    "            initial_learning_rate = initial_learning_rate, \n",
    "            pretrained_model_path = h5_file_path,\n",
    "            L2_weighting_factor = L2_weighting_factor)\n",
    "\n",
    "# # Show info about the GPU memory useage\n",
    "# !nvidia-smi\n",
    "\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N7-ShZpLhwr"
   },
   "source": [
    "# **5. Evaluate your model**\n",
    "---\n",
    "\n",
    "<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n",
    "\n",
    "<font size = 4>**We highly recommend to perform quality control on all newly trained models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "JDRsm7uKoBa-",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test model will be evaluated\n"
     ]
    }
   ],
   "source": [
    "# model name and path\n",
    "#@markdown ###Do you want to assess the model you just trained ?\n",
    "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ###If not, please provide the path to the model folder:\n",
    "#@markdown #####During training, the model files are automatically saved inside a folder named after the parameter `model_name` (see section 4.1). Provide the name of this folder as `QC_model_path` . \n",
    "\n",
    "QC_model_path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "if (Use_the_current_trained_model): \n",
    "  QC_model_path = os.path.join(model_path, model_name)\n",
    "\n",
    "if os.path.exists(QC_model_path):\n",
    "  print(\"The \"+os.path.basename(QC_model_path)+\" model will be evaluated\")\n",
    "else:\n",
    "  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n",
    "  print('Please make sure you provide a valid model path before proceeding further.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw7KaHZUoHC4"
   },
   "source": [
    "## **5.1. Inspection of the loss function**\n",
    "---\n",
    "\n",
    "<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n",
    "\n",
    "<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n",
    "\n",
    "<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n",
    "\n",
    "<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n",
    "\n",
    "<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "qUc-JMOcoGNZ",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n",
    "\n",
    "\n",
    "lossDataFromCSV = []\n",
    "vallossDataFromCSV = []\n",
    "\n",
    "with open(os.path.join(QC_model_path,'Quality Control/training_evaluation.csv'),'r') as csvfile:\n",
    "    csvRead = csv.reader(csvfile, delimiter=',')\n",
    "    next(csvRead)\n",
    "    for row in csvRead:\n",
    "      if row:\n",
    "        lossDataFromCSV.append(float(row[0]))\n",
    "        vallossDataFromCSV.append(float(row[1]))\n",
    "\n",
    "epochNumber = range(len(lossDataFromCSV))\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n",
    "plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n",
    "plt.title('Training loss and validation loss vs. epoch number (linear scale)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n",
    "plt.semilogy(epochNumber,vallossDataFromCSV, label='Validation loss')\n",
    "plt.title('Training loss and validation loss vs. epoch number (log scale)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend()\n",
    "#plt.savefig(os.path.join(QC_model_path,'Quality Control/lossCurvePlots.png'), bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTRou0izLjhd"
   },
   "source": [
    "# **6. Using the trained model**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAf8aBDmWTx7"
   },
   "source": [
    "## **6.1 Generate image prediction and localizations from unseen dataset**\n",
    "---\n",
    "\n",
    "<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n",
    "\n",
    "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
    "\n",
    "<font size = 4>**`Result_folder`:** This folder will contain the found localizations csv.\n",
    "\n",
    "<font size = 4>**`batch_size`:** This paramter determines how many frames are processed by any single pass on the GPU. A higher `batch_size` will make the prediction faster but will use more GPU memory. If an OutOfMemory (OOM) error occurs, decrease the `batch_size`. **DEFAULT: 4**\n",
    "\n",
    "<font size = 4>**`threshold`:** This paramter determines threshold for local maxima finding. The value is expected to reside in the range **[0,1]**. A higher `threshold` will result in less localizations. **DEFAULT: 0.1**\n",
    "\n",
    "<font size = 4>**`neighborhood_size`:** This paramter determines size of the neighborhood within which the prediction needs to be a local maxima in recovery pixels (CCD pixel/upsampling_factor). A high `neighborhood_size` will make the prediction slower and potentially discard nearby localizations. **DEFAULT: 3**\n",
    "\n",
    "<font size = 4>**`use_local_average`:** This paramter determines whether to locally average the prediction in a 3x3 neighborhood to get the final localizations. If set to **True** it will make inference slightly slower depending on the size of the FOV. **DEFAULT: True**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "7qn06T_A0lxf",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test model will be used.\n",
      "Using local averaging\n",
      "------------------------------------\n",
      "Running prediction on: Widefield_0878_test.tif\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning prediction on: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m filename)\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mbatchFramePredictionLocalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mData_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mResult_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mneighborhood_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43muse_local_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(Data_folder):\n\u001b[1;32m     66\u001b[0m     batchFramePredictionLocalization(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(Data_folder), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(Data_folder), prediction_model_path, Result_folder, \n\u001b[1;32m     67\u001b[0m                                    batch_size, \n\u001b[1;32m     68\u001b[0m                                    threshold, \n\u001b[1;32m     69\u001b[0m                                    neighborhood_size, \n\u001b[1;32m     70\u001b[0m                                    use_local_average, \n\u001b[1;32m     71\u001b[0m                                    pixel_size \u001b[38;5;241m=\u001b[39m pixel_size)\n",
      "Cell \u001b[0;32mIn[1], line 631\u001b[0m, in \u001b[0;36mbatchFramePredictionLocalization\u001b[0;34m(dataPath, filename, modelPath, savePath, batch_size, thresh, neighborhood_size, use_local_avg, pixel_size)\u001b[0m\n\u001b[1;32m    628\u001b[0m pixel_size_hr \u001b[38;5;241m=\u001b[39m pixel_size\u001b[38;5;241m/\u001b[39mupsampling_factor\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# get dataset dimensions\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m (nFrames, M, N) \u001b[38;5;241m=\u001b[39m Images\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(N)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(M)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(nFrames)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m frames.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Build the model for a bigger image\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------- User input -------------------------------\n",
    "#@markdown ### Data parameters\n",
    "Data_folder = \"DEEPSTORM_test_data_infinite_SNR/\" #@param {type:\"string\"}\n",
    "Result_folder = \"DEEPSTORM_test_data_infinite_SNR/\" #@param {type:\"string\"}\n",
    "#@markdown Get pixel size from file?\n",
    "get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, use this value (in nm):\n",
    "pixel_size = 50 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### Model parameters\n",
    "#@markdown Do you want to use the model you just trained?\n",
    "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, please provide path to the model folder below\n",
    "prediction_model_path = \"models_infinite_SNR/test/weights_best.hdf5\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Prediction parameters\n",
    "batch_size =  4#@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Post processing parameters\n",
    "threshold =  0.1#@param {type:\"number\"}\n",
    "neighborhood_size =  5#@param {type:\"integer\"}\n",
    "#@markdown Do you want to locally average the model output with CoG estimator ?\n",
    "use_local_average = True #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "if get_pixel_size_from_file:\n",
    "  pixel_size = None\n",
    "\n",
    "if (Use_the_current_trained_model): \n",
    "  prediction_model_path = os.path.join(model_path, model_name)\n",
    "\n",
    "if os.path.exists(prediction_model_path):\n",
    "  print(\"The \"+os.path.basename(prediction_model_path)+\" model will be used.\")\n",
    "else:\n",
    "  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n",
    "  print('Please make sure you provide a valid model path before proceeding further.')\n",
    "\n",
    "# inform user whether local averaging is being used\n",
    "if use_local_average == True: \n",
    "  print('Using local averaging')\n",
    "\n",
    "if not os.path.exists(Result_folder):\n",
    "  print('Result folder was created.')\n",
    "  os.makedirs(Result_folder)\n",
    "\n",
    "\n",
    "# ------------------------------- Run predictions -------------------------------\n",
    "\n",
    "start = time.time()\n",
    "#%% This script tests the trained fully convolutional network based on the \n",
    "# saved training weights, and normalization created using train_model.\n",
    "\n",
    "if os.path.isdir(Data_folder): \n",
    "  for filename in list_files(Data_folder, 'tif'):\n",
    "    # run the testing/reconstruction process\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Running prediction on: \"+ filename)\n",
    "    batchFramePredictionLocalization(Data_folder, filename, prediction_model_path, Result_folder, \n",
    "                                     batch_size, \n",
    "                                     threshold, \n",
    "                                     neighborhood_size, \n",
    "                                     use_local_average,\n",
    "                                     pixel_size = pixel_size)\n",
    "\n",
    "elif os.path.isfile(Data_folder):\n",
    "    batchFramePredictionLocalization(os.path.dirname(Data_folder), os.path.basename(Data_folder), prediction_model_path, Result_folder, \n",
    "                                   batch_size, \n",
    "                                   threshold, \n",
    "                                   neighborhood_size, \n",
    "                                   use_local_average, \n",
    "                                   pixel_size = pixel_size)\n",
    "\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "\n",
    "# ------------------------------- Interactive display -------------------------------\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "print('---------------------------- Previews ------------------------------')\n",
    "print('--------------------------------------------------------------------')\n",
    "\n",
    "if os.path.isdir(Data_folder): \n",
    "  @interact\n",
    "  def show_QC_results(file = list_files(Data_folder, 'tif')):\n",
    "\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "    # Wide-field\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+file))\n",
    "    plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n",
    "    plt.title('Widefield', fontsize=15)\n",
    "    # Prediction\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+file))\n",
    "    plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n",
    "    plt.title('Predicted',fontsize=15)\n",
    "\n",
    "if os.path.isfile(Data_folder):\n",
    "\n",
    "  plt.figure(figsize=(15,7.5))\n",
    "  # Wide-field\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.axis('off')\n",
    "  img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+os.path.basename(Data_folder)))\n",
    "  plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n",
    "  plt.title('Widefield', fontsize=15)\n",
    "  # Prediction\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.axis('off')\n",
    "  img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+os.path.basename(Data_folder)))\n",
    "  plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n",
    "  plt.title('Predicted',fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'shape4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mImages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape4\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'shape4'"
     ]
    }
   ],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 120, 40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(Data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Deep-STORM_2D_ZeroCostDL4Mic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "082403d267e14b4988a1835f64a16a5f": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_4e01321bb3bc4114a51248efc09e8d64",
      "msg_id": "",
      "outputs": []
     }
    },
    "183d1e66afbb4fbc9cce8982f9190e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a3105aa8c0ba42ea95a8f87803250b3a",
       "IPY_MODEL_338619ea174a46df80db3bfdbded16e0"
      ],
      "layout": "IPY_MODEL_f5aedf72d7154efc831589e438d1247b"
     }
    },
    "1a047ddbd7374e35a739a6c3b55728f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "276718597a1e437d9306f056154efe42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f03eafa3bb0459bb6bed9c1f0d2b855",
       "IPY_MODEL_082403d267e14b4988a1835f64a16a5f"
      ],
      "layout": "IPY_MODEL_3b6433ecff6b4b3ca4c39209acb78933"
     }
    },
    "338619ea174a46df80db3bfdbded16e0": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_9c2c6001c74f4a1c819f5346a1f9b90f",
      "msg_id": "",
      "outputs": []
     }
    },
    "3b6433ecff6b4b3ca4c39209acb78933": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "468ee561c8ee43d896a9f06578ca43f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "4e01321bb3bc4114a51248efc09e8d64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cd5905f042a4e08a0e10757487db630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c2c6001c74f4a1c819f5346a1f9b90f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f03eafa3bb0459bb6bed9c1f0d2b855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "patch",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_5cd5905f042a4e08a0e10757487db630",
      "max": 10000,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_468ee561c8ee43d896a9f06578ca43f8",
      "value": 1
     }
    },
    "9fa05e2ecc6b48cd9e906affce90a5c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3105aa8c0ba42ea95a8f87803250b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "frame",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_9fa05e2ecc6b48cd9e906affce90a5c0",
      "max": 20,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_1a047ddbd7374e35a739a6c3b55728f2",
      "value": 1
     }
    },
    "f5aedf72d7154efc831589e438d1247b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
