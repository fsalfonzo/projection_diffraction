{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52250396-ace5-406f-832d-8cfc62991f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psfmodels as psfm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import PowerNorm\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import pyclesperanto_prototype as cle\n",
    "from pyclesperanto_prototype import imshow\n",
    "import pandas as pd\n",
    "from skimage.transform import rescale, resize, downscale_local_mean, rotate\n",
    "from scipy.optimize import minimize, LinearConstraint, Bounds, differential_evolution, basinhopping, shgo, dual_annealing, direct\n",
    "from skimage.filters import gaussian\n",
    "from scipy.optimize import LinearConstraint\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_midline(arr):\n",
    "    mid_x = arr.shape[1]//2\n",
    "    return arr[mid_x]\n",
    "\n",
    "def get_midline_rotate_method(arr):\n",
    "    mid_x = arr.shape[1]//2\n",
    "    midline = np.array([rotate(arr, angle, resize = False, clip = True, preserve_range = True)[mid_x] for angle in range(366)]).mean(axis=0)\n",
    "    return midline\n",
    "\n",
    "def MSLE(arr1, arr2): # MEAN SQUARE LOG ERROR\n",
    "    return np.nanmean((np.log(arr1 + 1) - np.log(arr2 + 1))**2)\n",
    "\n",
    "def MSE(arr1, arr2):\n",
    "    return np.nanmean((arr1 - arr2)**2)\n",
    "\n",
    "def MLSE(arr1, arr2): # MEAN LOG SQUARE ERROR\n",
    "    return np.nanmean(np.log((arr1 - arr2)**2 + 1))\n",
    "\n",
    "def RMLSE(arr1, arr2):\n",
    "    return np.sqrt(MLSE(arr1,arr2))\n",
    "\n",
    "def RMSLE(arr1, arr2):\n",
    "    return np.sqrt(MSLE(arr1, arr2))\n",
    "\n",
    "loss_fns = [MSLE, MSE, MLSE, RMLSE, RMSLE]\n",
    "\n",
    "def lorentzian_2d_DoG(amplitude, xo, yo, gamma_x, gamma_y, sigma1, sigma2, offset, amplitude_gauss1, amplitude_gauss2, xdata_tuple):\n",
    "    (x, y) = xdata_tuple\n",
    "    lorentz =  amplitude / (1 + ((x - xo) / gamma_x)**2 + ((y - yo) / gamma_y)**2) + offset\n",
    "    gaussian_term_1 = amplitude_gauss1 * np.exp(-((x - xo)**2 + (y - yo)**2) / (2 * sigma1**2)) + offset * 10\n",
    "    gaussian_term_2 = amplitude_gauss2 * np.exp(-((x - xo)**2 + (y - yo)**2) / (2 * sigma2**2))\n",
    "    DoG = (gaussian_term_1 - gaussian_term_2)\n",
    "    DoG = DoG - min(0, DoG.min())\n",
    "    DoG = DoG\n",
    "    return lorentz * DoG \n",
    "\n",
    "def symmetric_moffat_DoG(amplitude, xo, yo, alpha_x, alpha_y, beta, offset, sigma1, sigma2, amplitude_gauss1, amplitude_gauss2, xdata_tuple):\n",
    "    (x, y) = xdata_tuple\n",
    "    moffat_term = amplitude * (1 + ((x - xo)**2 / alpha_x**2) + ((y - yo)**2 / alpha_x**2)) ** (-beta)# + offset\n",
    "    gaussian_term_1 = amplitude_gauss1 * np.exp(-((x - xo)**2 + (y - yo)**2) / (2 * sigma1**2)) + offset\n",
    "    gaussian_term_2 = amplitude_gauss2 * np.exp(-((x - xo)**2 + (y - yo)**2) / (2 * sigma2**2))\n",
    "    DoG = (gaussian_term_1 - gaussian_term_2)\n",
    "    DoG = DoG - min(0, DoG.min())\n",
    "    \n",
    "    return moffat_term * DoG\n",
    "\n",
    "def check_zero_cost(intermediate_result):\n",
    "    if intermediate_result.fun == 0:\n",
    "        print(f\"Cost is zero for parameters: {intermediate_result.x}\")\n",
    "    return False  # return True if you want to stop the optimization here\n",
    "\n",
    "def perc_err(act,exp):\n",
    "    return abs((act - exp)/exp) * 100\n",
    "\n",
    "def get_midline_rotate_method(arr):\n",
    "    mid_x = arr.shape[1]//2\n",
    "    midline = np.array([rotate(arr, angle, resize = False, clip = True, preserve_range = True)[mid_x] for angle in range(366)]).mean(axis=0)\n",
    "    return midline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c5e9fd-9f6c-43e5-be6b-855d5d1bae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_moffat(real_PSF, error_function, xdata_tuple):\n",
    "    param_bounds = {\n",
    "        'amplitude': (0, 5),\n",
    "        'xo': (0, real_PSF.shape[1]),\n",
    "        'yo': (0, real_PSF.shape[0]),\n",
    "        'alpha_x': (0.001, 50),\n",
    "        'alpha_y': (0.001, 50),\n",
    "        'beta': (0.01, 20,),\n",
    "        'offset': (0, 3),\n",
    "        'sigma1': (1, 50),\n",
    "        'sigma2': (1, 50),\n",
    "        'amplitude_gauss1': (0, 10),\n",
    "        'amplitude_gauss2': (0, 10),\n",
    "    }\n",
    "    \n",
    "    # Create bounds for optimization\n",
    "    lower_bounds = [param_bounds[name][0] for name in param_bounds]\n",
    "    upper_bounds = [param_bounds[name][1] for name in param_bounds]\n",
    "    bounds = Bounds(lower_bounds, upper_bounds)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the constraint matrix\n",
    "    A = np.array([\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1],  # amplitude_gauss2 - amplitude_gauss1  > 0\n",
    "        [0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0]   # sigma2 - sigma1 > 0 (equivalent to sigma1 < sigma2)\n",
    "    ])\n",
    "    \n",
    "    # Define the lower and upper bounds for the inequalities\n",
    "    lb = [np.finfo(float).eps, np.finfo(float).eps]  # Lower bounds slightly above 0\n",
    "    ub = [np.inf, np.inf]                            # Upper bounds are infinity\n",
    "    \n",
    "    # Create the LinearConstraint object\n",
    "    constraints = LinearConstraint(A, lb, ub)\n",
    "\n",
    "\n",
    "    result = differential_evolution(error_function, bounds, workers = 24, disp = False, callback = check_zero_cost, args = (xdata_tuple, real_PSF))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f48de1-7069-4c04-8206-512760dac63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_PSF(PSF_size_xyz):\n",
    "    pix_mic_conv = 0.05\n",
    "    real_PSF = tifffile.imread(\"../PSF_comparison/average_100x_PSF_postmag_green.tif\").astype(np.float32)\n",
    "    real_PSF = rescale(real_PSF, (0.05/pix_mic_conv, (0.0655/1.5) / pix_mic_conv, (0.0655/1.5) / pix_mic_conv) , anti_aliasing=True)\n",
    "    #real_PSF = gaussian(real_PSF, sigma = 0.75)\n",
    "    midpoint_xy = real_PSF.shape[1]//2\n",
    "    \n",
    "    original_z = real_PSF.shape[0]\n",
    "    original_xy = real_PSF.shape[1]\n",
    "    \n",
    "    max_intensities = real_PSF.max(axis=(1,2))\n",
    "    peak_intensity_z =  np.argwhere(max_intensities == np.max(max_intensities))[0][0]\n",
    "    \n",
    "    \n",
    "    real_PSF = real_PSF[\n",
    "        :, # 23 Z stacks total\n",
    "        midpoint_xy - PSF_size_xyz//2:midpoint_xy + PSF_size_xyz//2+1,\n",
    "        midpoint_xy - PSF_size_xyz//2:midpoint_xy + PSF_size_xyz//2+1\n",
    "    ]\n",
    "    \n",
    "    real_PSF = real_PSF[peak_intensity_z]\n",
    "    real_PSF -= np.round(np.sort(real_PSF.flatten())[0:int(len(real_PSF.flatten())*.05)].mean()).astype(np.uint)\n",
    "    real_PSF = np.clip(real_PSF, 0, np.inf)\n",
    "    real_PSF = real_PSF / real_PSF.max()\n",
    "    return real_PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36eeaeac-2228-4598-9759-73599e3a9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lorentz(real_PSF, error_function, xdata_tuple):\n",
    "    param_bounds = {\n",
    "        'amplitude': (0, 5),\n",
    "        'xo': (0, real_PSF.shape[1]),\n",
    "        'yo': (0, real_PSF.shape[0]),\n",
    "        'gamma_x': (0.001, 50),\n",
    "        'gamma_y': (0.001, 50),\n",
    "        'sigma1': (1, 20),\n",
    "        'sigma2': (1, 20),\n",
    "        'offset': (0, 3),\n",
    "        'amplitude_gauss1': (0, 10),\n",
    "        'amplitude_gauss2': (0, 10),\n",
    "    }\n",
    "    \n",
    "    # Create bounds for optimization\n",
    "    lower_bounds = [param_bounds[name][0] for name in param_bounds]\n",
    "    upper_bounds = [param_bounds[name][1] for name in param_bounds]\n",
    "    bounds = Bounds(lower_bounds, upper_bounds)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the constraint matrix\n",
    "    A = np.array([\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 1, -1],  # amplitude_gauss2 - amplitude_gauss1  > 0\n",
    "        [0, 0, 0, 0, 0, 1, -1, 0, 0, 0]   # sigma2 - sigma1 > 0 (equivalent to sigma1 < sigma2)\n",
    "    ])\n",
    "    \n",
    "    # Define the lower and upper bounds for the inequalities\n",
    "    lb = [np.finfo(float).eps, np.finfo(float).eps]  # Lower bounds slightly above 0\n",
    "    ub = [np.inf, np.inf]                            # Upper bounds are infinity\n",
    "\n",
    "    # Define the constraint matrix\n",
    "    A = np.array([\n",
    "        [0, 0, 0, 0, 0, 1, -1, 0, 0, 0]   # sigma2 - sigma1 > 0 (equivalent to sigma1 < sigma2)\n",
    "    ])\n",
    "    \n",
    "    # Define the lower and upper bounds for the inequalities\n",
    "    lb = [np.finfo(float).eps]  # Lower bounds slightly above 0\n",
    "    ub = [np.inf]    \n",
    "    \n",
    "    # Create the LinearConstraint object\n",
    "    constraints = LinearConstraint(A, lb, ub)\n",
    "\n",
    "\n",
    "    result = differential_evolution(error_function, bounds, workers = 24, disp = False, callback = check_zero_cost, args = (xdata_tuple, real_PSF), constraints = constraints, updating='deferred')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deead7d8-9034-45ef-a3b6-5680528aedb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c9b48b-37aa-430d-abdb-d9500c0b3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_PSF(PSF_size_xyz, parameterised_function, fit_function, error_function):\n",
    "\n",
    "    real_PSF = get_real_PSF(PSF_size_xyz)\n",
    "    x = np.linspace(0, real_PSF.shape[1]-1, real_PSF.shape[1])\n",
    "    y = np.linspace(0, real_PSF.shape[0]-1, real_PSF.shape[0])\n",
    "    x, y = xdata_tuple = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    result = fit_function(real_PSF, error_function, xdata_tuple)\n",
    "\n",
    "    #print(result)\n",
    "    best_fit = parameterised_function(*result.x, (x,y))\n",
    "    \n",
    "    MSE_loss = MSLE(best_fit, real_PSF)\n",
    "\n",
    "    #plt.plot(best_fit[best_fit.shape[0]//2])\n",
    "    #plt.plot(get_midline_rotate_method(real_PSF), label = \"real\")\n",
    "\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.title(\"Fit\")\n",
    "    #plt.show()\n",
    "    \n",
    "    return result, MSE_loss, get_midline_rotate_method(best_fit)\n",
    "\n",
    "def get_extrapolated_fit(result, original_fit_xy, parameterised_function, error_function):\n",
    "    real_PSF = get_real_PSF(201)\n",
    "    x = np.linspace(0, real_PSF.shape[1]-1, real_PSF.shape[1])\n",
    "    y = np.linspace(0, real_PSF.shape[0]-1, real_PSF.shape[0])\n",
    "    x, y = xdata_tuple = np.meshgrid(x, y)\n",
    "    best_fit_result = deepcopy(result.x)\n",
    "    #best_fit_result[1] = best_fit_result[2] = real_PSF.shape[1]/2\n",
    "    best_fit_result[2] = best_fit_result[2] + real_PSF.shape[1]/2 - original_fit_xy/2\n",
    "    best_fit_result[1] = best_fit_result[1] + real_PSF.shape[1]/2 - original_fit_xy/2\n",
    "    best_fit = parameterised_function(*best_fit_result, (x,y))\n",
    "\n",
    "    #plt.plot(best_fit[best_fit.shape[0]//2])\n",
    "    #plt.plot(get_midline_rotate_method(real_PSF), label = \"real\")\n",
    "    #plt.plot(real_PSF[real_PSF.shape[0]//2], label = \"real\")\n",
    "\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.title(\"Extrapolation\")\n",
    "    #plt.show()\n",
    "    MSE_loss = MSLE(best_fit, real_PSF)\n",
    "    return error_function(best_fit_result, xdata_tuple, real_PSF), MSE_loss, get_midline_rotate_method(best_fit), get_midline_rotate_method(real_PSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fa2c24-a4dd-4c57-9ce7-30686777baf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.1450752472661426e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 1.7569975923700702e-05\n",
      "                   x: [ 4.953e+00  2.467e+01  2.443e+01  2.814e+00\n",
      "                        2.801e+00  1.731e+00  1.566e+00  5.330e-03\n",
      "                        1.106e+00  9.567e-01]\n",
      "                 nit: 614\n",
      "                nfev: 79641\n",
      "          population: [[ 3.506e+00  2.466e+01 ...  5.289e-01  3.051e-01]\n",
      "                       [ 3.475e+00  2.467e+01 ...  4.960e-01  2.616e-01]\n",
      "                       ...\n",
      "                       [ 3.350e+00  2.466e+01 ...  5.150e-01  2.834e-01]\n",
      "                       [ 3.378e+00  2.467e+01 ...  5.248e-01  2.987e-01]]\n",
      " population_energies: [ 1.757e-05  1.847e-05 ...  1.849e-05  1.847e-05]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n",
      "                 jac: [array([[ 0.000e+00,  0.000e+00,  0.000e+00,\n",
      "                               0.000e+00,  0.000e+00,  1.000e+00,\n",
      "                              -1.000e+00,  0.000e+00,  0.000e+00,\n",
      "                               0.000e+00]]), array([[ 1.000e+00,  0.000e+00, ...,  0.000e+00,\n",
      "                               0.000e+00],\n",
      "                             [ 0.000e+00,  1.000e+00, ...,  0.000e+00,\n",
      "                               0.000e+00],\n",
      "                             ...,\n",
      "                             [ 0.000e+00,  0.000e+00, ...,  1.000e+00,\n",
      "                               0.000e+00],\n",
      "                             [ 0.000e+00,  0.000e+00, ...,  0.000e+00,\n",
      "                               1.000e+00]])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.1178861228816329e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 1.9790753908621975e-05\n",
      "                   x: [ 3.676e+00  2.466e+01  2.442e+01  3.187e+00\n",
      "                        3.248e+00  1.827e+00  1.503e+00  5.697e-03\n",
      "                        6.993e-01  4.796e-01]\n",
      "                 nit: 566\n",
      "                nfev: 89998\n",
      "          population: [[ 3.676e+00  2.466e+01 ...  6.993e-01  4.796e-01]\n",
      "                       [ 3.273e+00  2.467e+01 ...  8.035e-01  5.694e-01]\n",
      "                       ...\n",
      "                       [ 3.749e+00  2.466e+01 ...  6.675e-01  4.474e-01]\n",
      "                       [ 3.493e+00  2.467e+01 ...  7.547e-01  5.271e-01]]\n",
      " population_energies: [ 1.979e-05  2.066e-05 ...  1.989e-05  2.005e-05]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.1164670690473464e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 2.1164095692592924e-05\n",
      "                   x: [ 2.207e+00  2.467e+01  2.441e+01  3.893e+00\n",
      "                        3.846e+00  1.970e+00  1.326e+00  6.674e-03\n",
      "                        6.375e-01  2.458e-01]\n",
      "                 nit: 439\n",
      "                nfev: 71336\n",
      "          population: [[ 2.207e+00  2.467e+01 ...  6.375e-01  2.458e-01]\n",
      "                       [ 2.253e+00  2.467e+01 ...  5.856e-01  2.044e-01]\n",
      "                       ...\n",
      "                       [ 2.213e+00  2.468e+01 ...  6.234e-01  2.367e-01]\n",
      "                       [ 2.236e+00  2.466e+01 ...  5.738e-01  1.852e-01]]\n",
      " population_energies: [ 2.116e-05  2.148e-05 ...  2.133e-05  2.145e-05]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.1078093096576476e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 0.004703564918498874\n",
      "                   x: [ 1.665e+00  2.466e+01  2.442e+01  3.996e+00\n",
      "                        4.069e+00  1.923e+00  1.378e+00  8.204e-03\n",
      "                        9.024e-01  3.746e-01]\n",
      "                 nit: 416\n",
      "                nfev: 65387\n",
      "          population: [[ 1.665e+00  2.466e+01 ...  9.024e-01  3.746e-01]\n",
      "                       [ 1.616e+00  2.466e+01 ...  8.856e-01  3.555e-01]\n",
      "                       ...\n",
      "                       [ 1.375e+00  2.467e+01 ...  8.542e-01  2.002e-01]\n",
      "                       [ 1.691e+00  2.466e+01 ...  8.928e-01  3.747e-01]]\n",
      " population_energies: [ 4.704e-03  4.727e-03 ...  4.856e-03  4.811e-03]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.0979256109318163e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 0.004923740851700243\n",
      "                   x: [ 1.113e+00  2.466e+01  2.440e+01  4.297e+00\n",
      "                        4.228e+00  1.824e+00  1.567e+00  1.057e-02\n",
      "                        2.248e+00  1.447e+00]\n",
      "                 nit: 408\n",
      "                nfev: 62899\n",
      "          population: [[ 1.113e+00  2.466e+01 ...  2.248e+00  1.447e+00]\n",
      "                       [ 1.074e+00  2.468e+01 ...  2.430e+00  1.574e+00]\n",
      "                       ...\n",
      "                       [ 1.102e+00  2.468e+01 ...  2.373e+00  1.556e+00]\n",
      "                       [ 1.025e+00  2.467e+01 ...  2.388e+00  1.505e+00]]\n",
      " population_energies: [ 4.924e-03  5.159e-03 ...  4.968e-03  4.973e-03]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.0915649451785609e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 2.042527207799461e-05\n",
      "                   x: [ 2.622e+00  2.466e+01  2.441e+01  3.407e+00\n",
      "                        3.422e+00  1.896e+00  1.421e+00  7.009e-03\n",
      "                        7.095e-01  3.949e-01]\n",
      "                 nit: 612\n",
      "                nfev: 87970\n",
      "          population: [[ 2.622e+00  2.466e+01 ...  7.095e-01  3.949e-01]\n",
      "                       [ 2.580e+00  2.466e+01 ...  7.013e-01  3.851e-01]\n",
      "                       ...\n",
      "                       [ 2.583e+00  2.467e+01 ...  7.334e-01  4.198e-01]\n",
      "                       [ 2.591e+00  2.466e+01 ...  7.843e-01  4.723e-01]]\n",
      " population_energies: [ 2.043e-05  2.049e-05 ...  2.077e-05  2.060e-05]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.0866702436698045e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 0.004568484588671332\n",
      "                   x: [ 2.569e+00  2.466e+01  2.441e+01  3.123e+00\n",
      "                        3.105e+00  1.960e+00  1.352e+00  8.253e-03\n",
      "                        6.421e-01  3.324e-01]\n",
      "                 nit: 425\n",
      "                nfev: 63653\n",
      "          population: [[ 2.569e+00  2.466e+01 ...  6.421e-01  3.324e-01]\n",
      "                       [ 2.593e+00  2.466e+01 ...  6.637e-01  3.633e-01]\n",
      "                       ...\n",
      "                       [ 2.715e+00  2.465e+01 ...  5.847e-01  2.963e-01]\n",
      "                       [ 2.568e+00  2.466e+01 ...  7.043e-01  3.944e-01]]\n",
      " population_energies: [ 4.568e-03  4.602e-03 ...  4.664e-03  4.595e-03]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKIElEQVR4nO3deVxVdf7H8ddlVURwB1FURFxww5XMGi011yzbG6fMtqm0X+VMi60zzUw2WU2LVJNN2bRpTWllm6WZS+aWmLuipqYCCgoCAsI9vz++slxBBblwuJf38/G4D+75nnPP/XCmiXfnfBeHZVkWIiIiIh7Cx+4CRERERCpD4UVEREQ8isKLiIiIeBSFFxEREfEoCi8iIiLiURReRERExKMovIiIiIhHUXgRERERj+JndwHu5nQ6OXDgAA0bNsThcNhdjoiIiFSAZVkcO3aMiIgIfHzOfG/F68LLgQMHiIyMtLsMEREROQf79u2jdevWZzzG68JLw4YNAfPLh4SE2FyNiIiIVERmZiaRkZHFf8fPxOvCS9GjopCQEIUXERERD1ORLh/qsCsiIiIeReFFREREPIrCi4iIiHgUhRcRERHxKAovIiIi4lEUXkRERMSjKLyIiIiIR1F4EREREY9SK8PL/Pnz6dSpEzExMbzxxht2lyMiIiK1SK2bYbegoIApU6bw/fffExoaSp8+fRg3bhxNmza1uzQRERGpBWrdnZdVq1bRtWtXWrVqRXBwMCNHjmTBggV2lyUiIiK1hNvDy5IlS7j00kuJiIjA4XAwb968MsckJCTQrl076tWrR3x8PKtWrSred+DAAVq1alW83apVK/bv3+/uMkVERMRDuT28ZGdn07NnTxISEsrdP2fOHKZMmcITTzzBzz//TM+ePRk+fDipqanuLkVERES8kNvDy8iRI/n73//OuHHjyt3//PPPc9tttzFx4kRiY2N57bXXCAoK4s033wQgIiLC5U7L/v37iYiIOO335eXlkZmZ6fISERER71WjfV7y8/NZu3YtQ4cOLSnAx4ehQ4eyYsUKAPr378/GjRvZv38/WVlZfPXVVwwfPvy055w2bRqhoaHFr8jISLNj+fJq/V1ERETEHjUaXg4fPkxhYSFhYWEu7WFhYSQnJwPg5+fHc889x0UXXURcXBx/+tOfzjjSaOrUqWRkZBS/9u3bZ3ZMmADqKyMiIuJ1at1QaYCxY8cyduzYCh0bGBhIYGBgmfaFwYcYd9VVsHgxlLNfREREPFON3nlp1qwZvr6+pKSkuLSnpKQQHh7u1u+6dWhjkjf+BPfd59bzioiIiL1qNLwEBATQp08fFi5cWNzmdDpZuHAhAwYMcOt3pS99lhvGgfO1V2H1areeW0REROzj9vCSlZVFYmIiiYmJAOzevZvExET27t0LwJQpU5g5cyZvv/02W7Zs4c477yQ7O5uJEydW6XsTEhKIjY2lX79+pmHrVXyXexXPDAQ+/rhK5xYREZHaw2FZluXOEy5evJiLLrqoTPuECROYNWsWADNmzGD69OkkJycTFxfHSy+9RHx8vFu+PzMzk9DQUCADgvLwuTOWrT81J2bZZrecX0RERNyv6O93RkYGISEhZzzW7eHFbsXhpdmPcHgADL+XJ4+/yGNzDoKb+9WIiIiIe1QmvNS6tY3cpse75mfSSL6MAbQ+koiIiFfw3vAS8yUM+itc/CgrW8Phbz+1uyIRERFxA68JL6d22K3fZD9c9BdotQbLAd/sWgBOp71FioiISJV5TXiZNGkSmzdvZvXJYdGDWw102f9leBb8/LMdpYmIiIgbeU14OdUlXS+DAn/Ychl8M52voqHw66/sLktERESqyGvDy7DoYeD0g48/gBV/5khWV1b9pPleREREPJ3XhpfI0Ei6hbaAqEWmYccovsz9BTIy7C1MREREqsRrwwvAqG5XmFFHADtG82W0BYsW2VuUiIiIVInXhJcyywMAo2Ivgw4n+7nsHcjPjUM5+O0nNlUoIiIi7uA14eXU0UYA50eeT0iTg9BsC1h+sGsYX+/4CrxrUmEREZE6xWvCS3n8ff25pNWFEPOFadgxii+apMH27fYWJiIiIufMq8MLwKg+15X0ezkSxYJoOPH1l/YWJSIiIufM68PLiA4joc0y+L/2MPEijgXC8hVz7C5LREREzpHXh5eWDVvSu0FraLK7uO3L7ET1exEREfFQXh9eAEZ1HO2y/XVkHqSk2FSNiIiIVEWdCC/De18NxxvB29/Cv35lU1Nf8jatt7ssEREROQdeE17Km+elSI+WcRCYAb8NgIy2OI90YNumJTVfpIiIiFSZ14SX8uZ5KRISGELbwiBovsk0pHZn496yx4mIiEjt5zXh5Wy6+beCsA1mI6U7G49orhcRERFPVGfCS9emnaHFRrOR2o1NTnXYFRER8UR1Jrx0izoPWpy885LanY0huVphWkRExAPVnfDS7eKSx0bp0ewKDiJ74zp7ixIREZFKqzPhpXNED3yCUk2n3XaL4XgTNm9cZHdZIiIiUkl+dhdQU+r71yc6vwE7JnUrbtu0Zw1lB1aLiIhIbeY1d17ONM9LkW7+rVy2Nx7ZVt1liYiIiJt5TXg50zwvRbo16VyycSKQjc7kGqhMRERE3MlrwktFdI3qD4dj4F+74cXdbGqQA7m5dpclIiIilVCnwku37kOh4QHIaAdZLfnNrxlHN621uywRERGphDoVXmLa9sLfLxsa7TINqd3YtGGhvUWJiIhIpdSp8BLgG0Cn3AauywTs0RpHIiIinqROhReArn4RLjPtbkrXiCMRERFPUufCS7cmnV3vvBQcsLcgERERqZS6F17a9S9ZoPFQVzYE5UBhob1FiYiISIXVufDStfsQaLodIpdBt9kcDqhP6laNOBIREfEUXrM8QEJCAgkJCRSe5S5K+w79qGcVkHvLhcVtm375jhZd+1d3iSIiIuIGXnPnpSIz7AL4+voRm9PApW3j7lXVWZqIiIi4kdeEl8ro5tvSvDkRCEfbsDF9i70FiYiISIXVyfDStXEn2D0YnsqC975gU8FBu0sSERGRCqqT4aVb237QeCdYfpDWiQ2BuVhOp91liYiISAXUzfDSfQiE7oPAo+D0J/NYJ/bvSrS7LBEREamAOhleImMH0DCfkvleUruxaf13ttYkIiIiFVMnw4vD15eu2UHQ7OTSAOkd2KwRRyIiIh6hToYXgI4+zaHxydWlj7Rn55Gd9hYkIiIiFVJnw0t0g8iS8JIezc5cjTgSERHxBF4zw25lRTeLgbCfIO5NaLmOnT5H7S5JREREKqDOhpf2kT0g4y24/BYAfi2EQmchvj6+NlcmIiIiZ1J3Hxt1PM9l+4Qv/PbbJpuqERERkYqqs+GleUwcwXlAQQAc7gjZzdi5dYXdZYmIiMhZeE14SUhIIDY2ln79+lXoeEe9ekRn+cP/ZsOMbbDpGnbtXV/NVYqIiEhVeU14qeiq0qW1LwxxHS6duq2aqhMRERF38Zrwci6iA8Jcw8uxPfYWJCIiImdVt8NLaJRLeNlVeMjegkREROSs6nR4aR/eBRrtNhtH2pPkl2VvQSIiInJWdTq8REf1gUa/mo38hmQUNCE9J83WmkREROTM6nR4adMlHl/fPGj4m2k40p6dSRXv8CsiIiI1r06HF//WbWmTAfR7BQY/Dg1S2LVT4UVERKQ2q7PLAwDg40N0bn12/25acdPO/RttLEhERETOpk7feQGIdjR12d6ZnmRTJSIiIlIRdfvOC9A+qBUUJsORKMhtxK5mv9ldkoiIiJxBnQ8v0U07wE4fePNHCP2VnXd2trskEREROQM9Nmrdo2SiusxI9vk5ySvIs7coEREROa06H17ad+gHwSnglwOWL2S04dc09XsRERGprep8eAnp2J1mOUDjkpl2d25faWtNIiIicnp1PrzQtCnRGT6uCzT+us7emkREROS0FF4cDtoXNCwJL0ej2Jm61d6aRERE5LQUXoBo/zDX1aUz99hbkIiIiJxWnR8qDRAd0hbaLIVBf4FWq9l5ItXukkREROQ0vCa8JCQkkJCQQGFhYaU/2z6sMzR4GSJMX5ddhQ6clhMfh25MiYiI1DZe89d50qRJbN68mdWrK7+wYnS73i7bub4WyVnJ7ipNRERE3MhrwktVtOzYm3ongCPtIGkYZDVn58HNdpclIiIi5VB4AXyi2hN1FPjkXXh3AewZxM6kVXaXJSIiIuVQeAEIDiY6K8B1xNH+DfbWJCIiIuVSeDkpmsauE9VpiQAREZFaSeHlpPb1I6BRqSUCcvbbW5CIiIiUy2uGSldVdJNoyD155yU9mp1Wmr0FiYiISLl05+Wk6FbdoclOs5HRhsMOJxm5GfYWJSIiImUovJwUFd0HR4MD4J8Nlh8ciSIpbYfdZYmIiMgpFF5OCozpQttMYOhDcPmN0OAQSfsS7S5LRERETqHwUqRNGzoccUD8DIh7B+ofJWnXGrurEhERkVMovBTx8yOmIMSlaUeyZtkVERGpbTTaqJQOgS0h1wn7+0NBfZI6rre7JBERETmFwkspHRpHQ0pTeOc7CP2VpOgudpckIiIip9Bjo1JiWveApidHGGW0IcWyyMzLtLcoERERcaHwUkpUx3gISoWATMDHzLSbvtPuskRERKQUhZdS6nWMpU0mJXdf0mM0XFpERKSWUXgprW1bYtKBJifDS1oMO3autrUkERERcaXwUlpAAB1ONHS986Lh0iIiIrWKwsspYgJbQtPtZiMthh2Zu+0tSERERFxoqPQpOjSOhkZL4LKJ0GIjSQWH7C5JRERESlF4OUWHVt0h6yvoNQuAZCArP4vggGBb6xIRERFDj41OER0Tj8NybUtKT7KnGBERESlD4eUU9TrG0joTONAb1twGyd1J+u0Xu8sSERGRkxReThUVRYd04Kd7Yf7rsGMUSRouLSIiUmvUyvAybtw4GjduzFVXXVXzXx4YSEx+cMlcL+kx7Di4qebrEBERkXLVyvByzz338N///te27+8QEF4y10taDEkaLi0iIlJr1MrwMnjwYBo2bGjb98c0ina585J0ItW2WkRERMRVpcPLkiVLuPTSS4mIiMDhcDBv3rwyxyQkJNCuXTvq1atHfHw8q1atcketNaZDRLeSOy9ZLTlQ4Et2fra9RYmIiAhwDuElOzubnj17kpCQUO7+OXPmMGXKFJ544gl+/vlnevbsyfDhw0lNLbl7ERcXR7du3cq8Dhw4cO6/iRu1j+kP9TLNCtMA6R3YeUSrS4uIiNQGlZ6kbuTIkYwcOfK0+59//nluu+02Jk6cCMBrr73GF198wZtvvslDDz0EQGJi4rlVW468vDzy8vKKtzMzM6t8zqCOXWm9FH5rugNyWphHRwc20COsR5XPLSIiIlXj1j4v+fn5rF27lqFDh5Z8gY8PQ4cOZcWKFe78qmLTpk0jNDS0+BUZGVn1k0ZHm+HSQx6GCYMh+ht2JGm4tIiISG3g1vBy+PBhCgsLCQsLc2kPCwsjOTm5wucZOnQoV199NV9++SWtW7c+Y/CZOnUqGRkZxa99+/adc/3F6tWjQ34DaLcEon6A+hkkHdxY9fOKiIhIldXKtY2+++67Ch8bGBhIYGCg22uI8Q8DdhVvJx3VcGkREZHawK3hpVmzZvj6+pKSkuLSnpKSQnh4uDu/qtp1CG0P+cmw8TrIiGTHhdPtLklERERw82OjgIAA+vTpw8KFC4vbnE4nCxcuZMCAAe78qmoXE9ENLB/47D/ww1/YfyKAnBM5dpclIiJS51U6vGRlZZGYmFg8Ymj37t0kJiayd+9eAKZMmcLMmTN5++232bJlC3feeSfZ2dnFo4+qS0JCArGxsfTr188t52sf0x8CsyD45PDttBh2Hdl15g+JiIhItav0Y6M1a9Zw0UUXFW9PmTIFgAkTJjBr1iyuvfZaDh06xOOPP05ycjJxcXF8/fXXZTrxutukSZOYNGkSmZmZhIaGVvl8DTp2I+JHONB0B2RFmDWODmykW4tubqhWREREzlWlw8vgwYOxLOuMx0yePJnJkyefc1G1Qvv2xKTDgeabYc8gONCX7UkrIe46uysTERGp02rl2ka1QoMGdDoeBFGLzPbOYazfv9bemkRERETh5Ux6OyIgaiHghEPdWLnvqN0liYiI1HleE17c3WEXoHfTbhB0BFqZ2XV37YvmWN4xt51fREREKs9hna0Di4cp6rCbkZFBSEhIlc6V+/orBO+fRGFqdwhOhuBDLJ24lAvaXOCmakVERAQq9/fba+68VId6/QbQNRUI3wDBhwBYt2u5vUWJiIjUcQovZ9K1K71TXS/Rz1sW2VSMiIiIgMLLmQUE0Nu3tXm/eRy8/R3ffOpZMwWLiIh4m1q5MGNt0iuiD7AXslrC7iEctBzkFuRSz6+e3aWJiIjUSbrzchY9uw/FYQHRC0zD3gtYtXuTrTWJiIjUZV4TXqpjqDRAw/4X0jENaJIEjXaDM4APP97j1u8QERGRivOa8DJp0iQ2b97M6tWr3XviLl3oneoLDorvvvywwGsum4iIiMfRX+Gz8fMr6bR7MrwkbYi1sSAREZG6TeGlAnq37G3eRC0CRyG5hzuy69cT9hYlIiJSRym8VECvbsPMm/pHod33EPMFiXt22VqTiIhIXaWh0hXQOH4QUethd2PgxmHggExeADrZXJmIiEjdozsvFdGpE70O+Zr3DvPj580L7atHRESkDvOa8FJdQ6UB8PUt6bR70rJNWXz5pfu/SkRERM5Mq0pX0FcPjGNUg3lmY/cgeHsRzZs72L3bQYMGbvsaERGROkmrSleD3t0uKdlosxwa7+bQIQevvGJfTSIiInWRwksFhZ03hIjMkxu+BfC7vwHwzDOQlWVfXSIiInWNwktFdehA70OlBmf1eJdGzQ9w+DDMmGFfWSIiInWNwktF+fi4dtr1LaTlwGcBmD4djh2zqS4REZE6RuGlEnqHx7lsH4h9hY4dLdLT4eWX7alJRESkrlF4qYQ+pTvtAhkBeYyfnESTJuDGgU0iIiJyBl4TXqp1npeTWp8/gi6HXNvyGv2T3bth8uRq+1oREREpxWvCy6RJk9i8eTOrV6+uvi9p144xhxq7NH25c77uuoiIiNQgrwkvNcLhYEw710dHiY4Ufsv8jYIC+PZb2LPHptpERETqCIWXSjp/9B00Ou7a9sWPs5gwAS65BP7zH3vqEhERqSsUXirJb+CFjNwX6NI2f+0HjB5t3s+eDd614IKIiEjtovBSWb6+jGlynkvTd3lbGToyh/r1YccO+Plnm2oTERGpAxRezsGIi2/Hx1mynevrZPXOeYwZY7Znz7anLhERkbpA4eUcNBkxjoH7XS/d/B/e4Prrzfs5c8DpLOeDIiIiUmUKL+eifn3G+HZxaZqf/hMjRliEhMC+ffDjjzbVJiIi4uUUXs7RmL6/d9n+LeA421NWMm6c2V6wwIaiRERE6gCFl3PUZdztRB1xbZu/YAYPPghr18Jf/2pPXSIiIt7Oa8JLTSwPUJqjWTPGZLdyaZv/6wK6dIHevcHhqJEyRERE6hyvCS81sjzAKcZ0HOOyvTLgEKmZB4u31WlXRETE/bwmvNhh0Lh7aZBfsm054MuvXyYjA266CaKiID//tB8XERGRc6DwUgWBHToz7JDrqoxvr3+bhg3hyy9h715YscKm4kRERLyUwksVXRMx1GV7ccABkn5bx7BhZlujjkRERNxL4aWKrpjwNE1zXNtmznmQ4cPN+2++qfmaREREvJnCSxUFRsVwU2Z7l7ZZRxbxu8G5gFnn6NAhOyoTERHxTgovbnDbsIdctg8HFvLT2un06GFWmP7uO5sKExER8UIKL27Q6fJbGHywnkvbv1cm6NGRiIhINVB4cQcfH/4YeblL0+L6KXTtvoU+faBbN3vKEhER8UYKL24y7uZnaJbt2rZh352sWQN//rM9NYmIiHgjhRc3CWwZyU15nV3aZh1bSl7+cZsqEhER8U4KL25026jHXLbT6jmZ+/HfycqCn36yqSgREREv4zXhpaYXZixPx0uu56LUBi5tLy7+giZNYPBgyMkp/3MiIiJScV4TXuxYmLEMh4Pb21/t0vRTy/U0bXKcvDxYutSmukRERLyI14SX2mLcTf+kRemOuw6oH/Y5oCHTIiIi7qDw4maBTVtwj+9Al7ZfO34EaJ0jERERd1B4qQZ33fY6DfNKtq32C3E4Ctm0CX77zb66REREvIHCSzVo1D6WO/N7lDQEHYEI0xdHd19ERESqRuGlmtw34TUCC0q2rQ5fA/DRRzYVJCIi4iUUXqpJePcBTDzWoaQh7m38B0/l+Wf321eUiIiIF1B4qUb3X/MCPs6TG41/5cTgp/no81ttrUlERMTTKbxUo/bnj+baIxEubS9lLCA764hNFYmIiHg+hZdq9tCop1y20/aOIK7XfmbPtqkgERERD6fwUs16jJjA6MNNShr29yMpqRsJM/LtK0pERMSDKbzUgEcveqJko/cb4Chk2fIAtmyxryYRERFPpfBSA84bdzeXHm5qNkL3Q0ezXMCLL2TYWJWIiIhnUnipCQ4Hfx/7Ag7r5HbffwMw678Ojh+3rywRERFPpPBSQ3oM+wPXH2llNqIXQKPd5OWG8O9X9tlbmIiIiIdReKlBT14/E79CwMcJfV4H4NkZmrRORESkMhRealD0eSO5NSvGbPR6ExqkkHzJ7SRtXGpvYSIiIh5E4aWGPXrzLOqdAIJTYVIshREbeOKdiXaXJSIi4jG8JrwkJCQQGxtLv3797C7ljFp1O5+78+PMRlA6AO8H7eT1lxdx5ZWQk2NfbSIiIp7AYVmWdfbDPEdmZiahoaFkZGQQEhJidznlStu9ifYzu5EZeLLhRD38X9jFieyWDBwIn38OjRvbWqKIiEiNqszfb6+58+JJmkZ15RH/ISUN/rmcuPYqgoJyWL4cLr8c8jUBr4iISLkUXmxyz5QPic7wLWlo8yPBV19Aw4YWS5bA3XeDd90TExERcQ+FF5sEhjbh2c7/59KWGrWOSy9/FIcDXn8dEhJsKk5ERKQWU3ix0WW3PsuQtFCXtrltnuKh+83EdffeC8uW2VCYiIhILabwYiOHjw//uvw1fJwlbcf9YZdzEDfcANdcA3362FefiIhIbaTwYrPuF1/HH7M7u7TNCd7Nzdf9h/feg/r1bSpMRESkllJ4qQWenPQ/GuW6tt298G4KTpQ0av4XERERQ+GlFmgW1ZW/BI9xadsYcpzn/nU1v/wCF14II0bYVJyIiEgto/BSS9x13wf0PBLo0vbXY/PJTlvFihWwdCls3mxTcSIiIrWIwkst4R8UzOvDXsRRam6XXH/4y/wxXHqpaZw506biREREahGFl1qk/+g/Mik71qVtQcghOrYzqeXttyE3t7xPioiI1B0KL7XMP/70BRFZrv+zvOU3idatCzhyBD7+2KbCREREagmFl1omJLwdL8e4zrx7KLiAsKhXATPzroiISF2m8FILjbv1OcamN3dpW9vvaXx8nCxZAlu22FSYiIhILaDwUgs5fHx4+dZPaFB6ZemQAwSf93fenJlO+/a2lSYiImI7hZdaqk33C3g6+HKXtsxLnmBp0kACA8v/jIiISF2g8FKL3XX/h1x8ysKNb9Xfyvw5T9pUkYiIiP0UXmoxHz9/3rz5UxrmlWo83ojrXznOTTdm2laXiIiInRRearm2cYN4oen4koasMLKWTOOd9wPJyrKvLhEREbsovHiAiff9l9HpzcxGs23QOAlnYSB/mfqWvYWJiIjYQOHFAzh8fJh511c0zgUcQMf5ALy01MnuzcttrU1ERKSmKbx4iJad+vJK6zvMxsnwciJpNNe9PpITecdtrExERKRmKbx4kOvueoUbjraFtksg4Bhkh7MqpxOPPjXE7tJERERqjMKLJ3E4SHh4GTHZFkR/Y9q2j+EZnxV88/E/7a1NRESkhtS68LJv3z4GDx5MbGwsPXr04KOPPrK7pFqlYfPWzB7xBr4d5kNAJhQGAHDD6oc5uHuDzdWJiIhUP4dlWZbdRZR28OBBUlJSiIuLIzk5mT59+rB9+3YaNGhQoc9nZmYSGhpKRkYGISEh1VytfZ7761X8ueAz8DtR3DYkoynfPHMQXz9/GysTERGpvMr8/a51d15atmxJXFwcAOHh4TRr1oz09HR7i6qFpjw6h0szG7m0LQxN4/G/X2xPQSIiIjWk0uFlyZIlXHrppUREROBwOJg3b16ZYxISEmjXrh316tUjPj6eVatWnVNxa9eupbCwkMjIyHP6vDdz+Pry5pQfiDjmA5kti9ufcixj7juP2FiZiIhI9ap0eMnOzqZnz54kJCSUu3/OnDlMmTKFJ554gp9//pmePXsyfPhwUlNTi4+Ji4ujW7duZV4HDhwoPiY9PZ0bb7yR119//Rx+rbqhILALPu+nwcs74ES94vYbtz7F1jVf21iZiIhI9alSnxeHw8HcuXO5/PLLi9vi4+Pp168fM2bMAMDpdBIZGcndd9/NQw89VKHz5uXlMWzYMG677TZuuOGGsx6bl1ey+E9mZiaRkZFe3+cFwLIgMhL27wfGj4SYksDSOTOAlQ/vJqRphH0FioiIVJBtfV7y8/NZu3YtQ4cOLfkCHx+GDh3KihUrKnQOy7K46aabuPjii88aXACmTZtGaGho8asuPWJyOGDMGPO+48bfu+zbGpLPTU/1x3I6bahMRESk+rg1vBw+fJjCwkLCwsJc2sPCwkhOTq7QOZYvX86cOXOYN28ecXFxxMXFsWHD6YcAT506lYyMjOLXvn37qvQ7eJqi8HI87ff0TK/nsm9uyH7+/tRwG6oSERGpPn52F3CqCy64AGcl7hYEBgYSGBhYjRXVbkOGQP36sO83X2bEL2HiL/Gk1y95Evh44Xd0evNPXHPzczZWKSIi4j5uvfPSrFkzfH19SUlJcWlPSUkhPDzcnV8lJ9WvD5ddZt7P/bYfH/R8EscpvZgm7Hqeld/OqvHaREREqoNbw0tAQAB9+vRh4cKFxW1Op5OFCxcyYMAAd36VlHLvvebn++9Dr6GP8lzAGJf9uf4w9rtb2LPlp5ovTkRExM0qHV6ysrJITEwkMTERgN27d5OYmMjevXsBmDJlCjNnzuTtt99my5Yt3HnnnWRnZzNx4kS3Fn6qhIQEYmNj6devX7V+T20UHw8PPQTffgvNmsG9D33KH491cjkmNcjJmJkXkZl24DRnERER8QyVHiq9ePFiLrroojLtEyZMYNasWQDMmDGD6dOnk5ycTFxcHC+99BLx8fFuKfhs6sryAGdz4ng2ox+K5NsmR1zaR2Q057Npe/APrG9TZSIiImVV5u93rVvbqKoUXsz8Lw4HHE3+lfOf6cSW0HyX/ROyY3jr6a04fGrd6hAiIlJHefTaRnLuDh+GKVPgwgtNgGkU3o75N3xFs+MOl+PebrCDqX+5wKYqRUREqkbhxYv4+sK//w3Ll8OiRaatfa+L+ezCV6h/wvXYf/qu4F/Tr6j5IkVERKrIa8JLXe6wW6RxYyjqF/2vf5W0Dxh9B3NiHsb3lOlzpuTM5b3XJ9dcgSIiIm6gPi9eZscO6NjRvN+2reQ9wFsvTuTmo7NcjvcrhM+6/p2R12olahERsY/6vNRhMTElSwbccYcJM0Um3vMW03yGuRxf4AtXbHiURXOfr8EqRUREzp3Cixd6+GHw84Pvv4cuXWDZspJ9Dz7yNfce7wn59eFQZ8hrQK4/XLrmTyz74lX7ihYREamgWre2kVTdgAGwZg088oi583LeeaY9LQ2uusqHHTvWwf6TI5D8cqDTZ+R0f5+RJ+7lu4B6xA+r3gkFRUREqkJ9XrxcRgaEhpr3BQVmLaSCArPt63ecwoJSk9X1eIfQ0Tey6JL36T34+povVkRE6iz1eZFiRcEFzKOkDz+EFSvMnDA5GU5+d/1wOO95CD4AXT4hIxCGfT2epV98yKFD9tUtIiJyOl5z5yUhIYGEhAQKCwvZvn277rxUUF52Jpc/Es3XoelgOcC3EIDABX/HN/F+HnoogPvug+Dgip0vKwu++w7WroWmTWHECOjcuRp/ARER8QpaHkCPjSrleEYaY57owKLGR02D0wGzfoC9FwLQoAGMGwfjx8PQoeYOTpETJ2D7dli4EL74AhYvhvxSqxG89x78/vfmfUEB7N8PbdvWyK8lIiIeRI+NpFLqhzblsye2MeRIY9PgY8FNg+DK63A02UZ2Nrz7LowcCRER8OmnJZ9dtQq6dYN77oEFC0xwiY6Gm2+Ga681o53ALFdw993Qu7eZAVhERORcKbwIAA0at+DzJ5MYeaSZafCxoPscrLs743/TAEYPW0WzZnDoECxdWvK5Dh3MI6XBg+HZZ2HrVjPC6T//gdmzoVcvc1xOjnmUlJ4OQ4bAxx/X+K8oIiJeQo+NxEVedibXPtqJTxslu7T7OuH15nfSMvoVcnLgyitNe9E/PQ4HZ5WdDddfD59/bo5/7jm47z43/wIiIuKR9NhIzllggxA+mraTazJau7QX+sAtaa+yef3Y4uACJoRUJLiA6Tszdy5MmmRCz5Qp8MorbixeRETqBIUXKcO/XhDvPb2DCZnty+z7c97nPPBoPJbTWc4nz87XF15+Gf7yF7P9yCNm2LaIiEhFeU140arS7uUXUI83n9nGn/P6lNk33X8VEx/syIncnHM6t8MBjz4KPXvC8ePw449VrVZEROoS9XmRM7Mspj81hgcKviyza8iRxvxv6joahZ3b2OfERDOJXlRUFWsUERGPpz4v4j4OB/c/8gWzmtyC7ylPihY2PsL5/+zI7g1Ly//sWcTFKbiIiEjlKbxIhUy4+w3mxTxG/ROu7VtC8znv3UGs/OY/VTr/ypXwww9VOoWIiNQRCi9SYWP+8CRLBs0iPNv1H5vUIIvBS29lzhv3ntN5P/rIrHx9yy2Ql+eGQkVExKspvEil9B02gZ8mLKFbRqBLe64/XLf/RR5+7HwKT+Sf5tPlGzECwsNh50546SV3VisiIt5I4UUqrW3XgSx7aDuXHGlaZt80vxWMfaA1Gal7K3y+hg1h2jTz/m9/g7Q0d1UqIiLeSOFFzkloizbMf3ovd2THltn3ZaND9P9nB7au/qrC57vxRjN0+tgxeP55d1YqIiLexmvCi+Z5qXn+9YJ49Z8bebXhdfgVuu7bHnKC/nNH8fFb91foXD4+JRPXvfSSJq4TEZHT0zwv4hZLPnuZq5bfw6Ggsv843ZfXm38+vhT/ekFnPIdlQZ8+sG4dTJ0KTz1VXdWKiEhto3lepMb9buzdrLlxGb2P1i+z71+BPzN4ajj7t6854zkcDnP3pVUrs1q1iIhIeXTnRdwqJ+MwdzzZn3dCdpfZ1+y4g3d7/53h1zx82s9blhkuXa9edVYpIiK1je68iG2CQpvx9vQk/h0ynoAC132H61uM2PII9z/Sj/zjWeV+3uFQcBERkTNTeBG3c/j4cPt97/Lj4HeIOuZXZv+zAWs4/+Ewdqz99rTnKCyEd9/VvC8iIlKWwotUmz5D/sDaP21n7NHwMvvWNsqh1yeX8HbCbVhOZ5n9330HN9wADz8Mhw7VRLUiIuIpFF6kWjVuGcW85/bzYv0ryjxGyg6Amw6/wVV/juTQ3i0u+y65xIw8ys6Ge+6BcvKNiIjUUQovUu0cPj783wMfs3LobDpnBJTZ/0noAboldOWzdx4p+YwDnnsO/Pzggw/gwQdrsmIREanNFF6kxsQNupY1j+3j1qxOZfalBllctuspbrm/I5mp+wAYNAj+c3Kx6mef1cy7IiJiKLxIjWrQuAUzp2/lf5F/pulxR5n9bwbvoMczUSye/TRglg345z/Nvj/9Cd5/vyarPb3sbDh+3O4qRETqJq8JL1oewLNcefN0Nt6eyKVHw8rs29OwkDnvToU//AHS0rj/frj3XvD1NXPA1KTCQvj2W1NKSkpJ+3//Cw0aQLt2cPXVsH9/zdYlIlKXaZI6sZXldPLWjFu4J2UWWSe7w0SnQ+JrEJwPNG8OM2bgvPJqfl7noG/fk5+zzGy8gwbBBRdAQNmuNFWSnQ0vvwyvvAL7zFMsnnsOpkwx7x94AKZPLzk+NhaWLIGmZRfaFhGRCqjM32+FF6kVdm9Yyk1vjWVZw6MseQsG7jvlgJEjYcYMaN8egMRE6NXL7AoKgvBwaNiw5HXllXDLLZWvIzcX/v1vs65Saqppa9QIfv97+OMfoUcP02ZZZvHIX36BCRPMnZf4eDPEOzj4XK6AiEjdphl2xeNEdb+Q7589zOKwBxiYGVr2gK++gq5dYdo0yM8nIMD0h2nRAnJyYNcuWL8eli0zh2Znl3x040bo0gXuvhu+/x4KCsqeHkwflthY84gqNdXkpFmz4OBBSEgoCS5gRkM1bw5DhsCCBdCkCaxcCVdcUfOPtkRE6hrdeZHa58ABuPNO+Oyz8vfHxprnOYMG4XTCtm2Qng5ZWXDsmHlddJHpjwImgEycWPLxpk1h7Fjo3NncwXnnHdOfBuDWW+Hrr+Gxx+Dmm8Hfv2Ilr1oFF18MISGwdClER5/j7y4iUkfpsZHCi+ezLPjoIzNDXXJy+cdce63peBIZecZTpaebQPHZZ/Dpp5CW5ro/MRF69iw5Nijo3NZXWrwY2rQpfrIlIiKVoPCi8OI9MjLg0UfNc5vy/lENCjJrCPzpTxVKHAUF5tHSJ5+YGzz9+pmRRK1aub/03FwtMikiUlEKLwov3mf1atNjdt268ve3b2/6w1x9temQYrPZs83IpMWLoWNHu6sREan91GFXvE+/fqZjSUICNG5cdv+uXeYx0nnnmTHLNnI6zYilgwfhmmvMHRgREXEfhRfxHH5+cNddsH27uQtT3h2WVavM5C9jx8LmzTVfI+DjA++9Z0YjrV8P991nSxkiIl5L4UU8T7Nm8NprsHYtDBxY/jGffw7du8Ptt5vOLTUsIsKMYgJT6ocf1ngJIiJeS+FFPFevXmYY0SeflN+xxOmEmTMhJgYef9yMoa5Bw4fD1Knm/a23QlJSjX69iIjXUngRz+ZwwLhxZia6V14xs9adKicH/vY306n3mWfMhDA15MknzfIFx46ZWX9zcmrsq0VEvJbXhBctzFjH+fubie2SkswMc0FBZY85fBgefBCiouDpp2vkToyfnxl5FBYGl1zi/jWYRETqIg2VFu904IBZufE//zGPj8rTtKmZH2byZLMgUjVKTzdLCIiISPk0VFokIgJefx02bIBLLy3/mLQ0M8Fdu3bwj39AZma1lVM6uOTnw86d1fZVIiJeT+FFvFtsrFkXYNUqGDOm/GPS080svu3amb4xp64f4EbJyTB4sHkdOlRtXyMi4tUUXqRu6NfPDJ9es8bMAVOeI0fMqKQ2bcwS1NVweyQoyGSj334zHXir8WaPiIjXUniRuqVPH7M649q1cNll5R+TkwMzZpjh11ddBT/95LavDwkxI7uDg80o7wsvhP373XZ6EZE6QeFF6qbevWHePLNW0rhx5R/jdMLHH8OAAWa887x5UFhY5a/u2tWseRQWBr/8Yk6/aVOVTysiUmcovEjdFhdnboUkJsL48eDrW/5xy5ebkNOlC7z6apXniunTB1asgE6dYN8+M1Hw8uVVOmWFWRbs3QvbtsGWLSY4bdhQ/qLdIiK1kYZKi5S2dy+89JIZqXSmeWAaNoQJE8zcMrGx5/x1aWnm6dWOHSbMtG9v2h9+2MwJExYGjRqZtSgbNzbrJUVFndvC2TNnmm4/P/5Yfp/kgoLTZzcRkepWmb/fCi8i5cnIgDfegBdeML1rz2TQILNg5OWXn9MsdLm58Ouv0Lmz2bYs0yfmdLPxDhoEixaZBSDL43Sax1FLlph+x0VBZ/x4eP99897f33yHw2HO43CYqXH8/Mz+L780T8r0fyERqSkKLwov4i4nTphVFZ991jxaOpPwcLjtNrMYZOvW5/yVBQVmAuB9+8xw6qNHzUCoI0dMwLj5ZrPYI5QElc2bzQrW69ebvsiHD5v9GzeaPjYAX39tHhOdf75ZFup0OWv3bvM4KyQEEhLg2mvP+VcREakwhReFF3E3y4KFC+Hll2H+/NPP2gvm2cull5qUMXJkye0MN0hNNV8dHm6216wxo8BPFRxs5pJ58kkTVCpj1Sq48UbTJwbMU7S7765S2SIiZ6XwovAi1WnPHtMnZubMs880FxYGN9wAEydWqW/M6UyeDG+/DT16QM+eJa/evau2jlJBgVk54aWXzPYTT5jXufS1ERGpCIUXhRepCXl5ZqTSK6/AsmVnP75/f7jpJrj+etML100sq3pChWXB3/9u5u0DmDTJhJnT9bUREakKhReFF6lpv/xihlC/8w5kZ5/52MBAM+x64kS4+GK3PlaqDq+8Yu7wWJa5y3PjjXZXJCLeSOFF4UXskpkJH3wAb70FK1ee/fgWLeCaa8zdmAEDau1zmQ8+gAULzCLduvMiItVB4UXhRWqDzZth1ixzNyY5+ezHt20L111ngkyPHrU2yICZAuerr0zuEhFxh8r8/dZ/Q4lUl9hYeOYZM+Z5/nyzEqO//+mP37MH/vlPM+tv165mhesdO2qs3IqyLDOtzbXXmkdIZ5rLT0SkOii8iFQ3Pz8YPRr+9z8zUcuLL5r1Ac5kyxbTU7ZjRzN86K9/rVVz+HfubB4fvfOO6YdcNKxaRKQmeM1jo4SEBBISEigsLGT79u16bCS13/btMHu26VCydWvFPhMdDVdcYV79+9vaAWXZMvOUa/9+s1rCf/9rJhkWETkX6vOiPi/iSSzLTI37wQcmzOzdW7HPRUSYtHDFFXDhhVWb2OUcpaSYfi9LlpjtRx4xN4m0RpKIVJbCi8KLeCqn06zQ+MEH8NFHZkrdimjYEC65BMaMMbP6hoVVb52lnDgBDzxgloFq0cLksKIZgEVEKkrhReFFvEFhISxfDnPnmsnwKnpHBsyaAWPGmL42vXrVyOOl994zSzoNGmS28/JMf+Nu3ar9q0XECyi8KLyIt7EsWLfOhJhPPjEdeisqPBxGjYLhw2HIEGjatPrqLOXVV82opHHjzKiknj2hXbtaPQJcRGyk8KLwIt5u61ZzR2buXFi9uuKfczjMSKdLLoFhw8wS09XUV+a++8zAqtL/hgkJge7dzd2YopWxwQSd7Gz43e/MjaIzjSg/Hcsyj7Bs6PojIm6g8KLwInVJcrKZMe6LL+CbbyArq+KfbdDAPOcpCjNdurj11sjmzWY9pJUrzfv8fNMeFOS6isKoUeZXKNp3/vmmD3J8vBlU1bhx+ec/fhwWLza/+hdfmDUwn3yyZP+vv5q7PSJS+ym8KLxIXZWfD0uXmknxvvii8pPchYXB4MElr06d3BZmTpww88GsX2+GVz/wQMm+//4XPv7YDL9OT3f9XOPGkJZWUsb118Nvv5m+zHv3Qm5uybHnnWf6OwMkJUFMjLnLM2KEeWJ2wQUQHOyWX0dE3EzhReFFxNi+Hb780ixMtHixuVVRGUVh5qKLzM+OHau104rTae7QLF1qgszq1dC+PXz9dckxrVub8FN6e/Roc/dmyBBzMwnMqPMbboCCgpJj/f3N3ZxLL4Vbb4UmTartVxGRSlJ4UXgRKSsvD3780QSZb7+Fn3+u/Iy94eHm9sXAgeZnXFy1r4p94oRrH5g5c8w8Mi1amKluoqNPn6fS003wWbjQvPbsKdn3zjvwhz9Ua+kiUgkKLwovImd3+LD5i14UZvbtq/w5goLMs5qiMHPeeaZXbi1kWbBrl/lVP/sM5s0r6dz7+efmDk/XrraWKFKnKbwovIhUTtFf9sWL4fvvzc/Sz2YqysfHDCc67zzT07Z/f9MJuBZPuVtQYPrGHDwIzz4LkyZpOLeIHRReFF5EqsayYOdOE2KKAs2BA+d2rgYNoG9fE2SKhg+1bl1rEkJmplmjqWi006hR8NZb5rGUiNQchReFFxH3siwzfGfZMvNavrxqS0mHh5fcmenf38wI3KiR28qtLMuCGTPg/vtN16AWLeC550zWiomxrSyROkXhReFFpPodOmQ6ABeFmTVrTO/ac9Whg5mhLi6u5GfLlu6qtkI2bjRDsTduNNsREa5Pz154waza0LYttGljBl/ZmLlEvIrCi8KLSM07ftwEmJ9+glWrzKsy6zGVJyzMhJjSgSYmplrXasrNhb/9zYwwb93adOYtEhlp5pgp4uNj+ilfdpl5RUdXW1kiXk/hReFFpHZITjaTtRSFmVWr4OjRqp2zQQPo0aMkzHTvDrGxNTLK6ZFHTL/mvXvNsOvSd2X69q3cSg0i4krhReFFpHYq6jtTOsysW2c6mlRVmzZmOt2uXUt+dulihnNXk19/NcOuP/vMrK7w4IOmPTcX/vUvuP32knUwMzPN7MJFj6aKHjelp5vL0rCh1mWSuk3hReFFxHPk58OGDSbEJCaa1/r1lVuj6XQcDjOBy6mhplMnCAys+vlP49//hjvuMLnpoovMOpo7d5bsT02F5s3N+8mTISHBvPf3NyEmONiswj12LFxzTa2dOkfErSrz97t6p8YUETmbgACz0nWfPiVtTqf5a18UaIp+JidX7txFQ7537oRPPy1p9/GBqCgTYopenTubn2FhVR7G3aaNeaq1bp1ZYqpI69YmlJReluDYsZL3J06YOzHp6ebR1Oefw8iRJeHl6FEIDa01o8xFbKM7LyLiOZKTS+7OrFtn7tAkJZkhQO4SEuIaaopeMTFQv36FT2NZZgLjTZvMzZ64OGjWrPxjT5wwq2wfO2ZuOKWnm6l1du40c84UGT4ctmwxazONHWuWm6rGG0giNUqPjRReROqOvDwz58zGjSYpFP3ctavyazedicNhbp1ER5th3aV/RkdX+7Od/HxzU6h0f+cGDeDii82q2SNHmptJIp5K4UXhRURycsxtilNDTVWHb59O8+YlQebUYNOihVue9Rw/bu7mfP45zJ/vOunx8OElq28XFMAbb5ibRb16afVs8QwKLwovInI6mZnmTs2pr+3bzTCh6lC/vukI066debVt6/ozPLzSc9dYFvzyi5mP5quv4NprzbpMYG46Fc054+trHi9deSWMG2e+SqQ2UnhReBGRynI6zV2Z8oJN6ZnpqkNAgAk3p4aaop8REeBX8fEV27bBn/9sRjklJZW0OxwwYAA8+SQMGWLa8vNNbqrE6UWqhcKLwouIuFNWFuzYUTJyKSmp5P2+fe7tW1MeX1/T36b0q1Ur1/ctW5qx1qdISoJPPoGPPzbT6oB57DRmjHn/4Ydw660waJAJNEOGmA7G1TiJsUi5PDq8HD16lKFDh1JQUEBBQQH33HMPt912W4U/r/AiIjUqN9fMVndqqElKgt27q7beU2U4HOaZ0KmhptT7vc7WLPyxPqNGmc6/AE89ZWYOLq1xYzPXDMCiRab7Dpg+N4GBCjZSPTw6vBQWFpKXl0dQUBDZ2dl069aNNWvW0LRomsqzUHgRkVqjsND0qt2zxwScU3/u3eue2YUro3Fjc5emZUsID6ewRUsSrR4sSu7Kwp1tWbqxMTnHS9LJli1mChwwIWfuXLP69vjxmhFY3Mujw0tp6enp9O7dmzVr1tDsdBMknELhRUQ8htMJKSmuoebUgJOTU6Ml5ePPFrpQ4B8ETZsSG3mM+hGNKQyLIHrOU+zJaARARPN8brk+hxtvDaRD94rPfyNyOtUaXpYsWcL06dNZu3YtBw8eZO7cuVx++eUuxyQkJDB9+nSSk5Pp2bMnL7/8Mv3796/wdxw9epRBgwaxY8cOpk+fzqSiLvQVoPAiIl7DsiAtreQuzf79pvPwb7+5vq+huzcZhPA6t/MC93KAVsXtA31+ZFLTOVzffiU0b87CgkE8seEqth1tQZOGBbSNyKddW2jX0Z/YPkEMG+VPgwY1UrJ4kGoNL1999RXLly+nT58+XHHFFWXCy5w5c7jxxht57bXXiI+P54UXXuCjjz5i27ZttGjRAoC4uDgKCgrKnHvBggVEREQUb6ekpHDFFVfwySefEFb0gPYsFF5EpE6xLDMlb1GQOTXYFG1nZrrtK/MI4BOu4G0m8C3DcOLLfTzP8/wJgO8YwjC+O+3n/xH4JA+3edfMjdOihZl6uGnT07+aNNFwqDqgxh4bORyOMuElPj6efv36MWPGDACcTieRkZHcfffdPPTQQ5X+jrvuuouLL76Yq666qtz9eXl55JX6r47MzEwiIyMVXkRESsvMNCFm/36zzEJyMhw86PozORmOHKnUaQ/QkvcYz2i+IJYtgLlDM58xxLKZDEL5lXb8Sjt2E8UyLmA+Y+jCVgD+x5VM534Gspzz+ZGBLKcl5axhFRp65oBT3qtBAy0E5UFsW5gxPz+ftWvXMnXq1OI2Hx8fhg4dyooVKyp0jpSUFIKCgmjYsCEZGRksWbKEO++887THT5s2jb/+9a9Vrl1ExKuFhJhXly5nPi431/TDKS/YlG5LSYETJ4jgIPfzrMspQslkPO+Xavmh+J0FlI4TH3E1q4hnFfH8iykAtCCFIHKoRy7LGUgTjkBGBq9k/J4Fuy4hkDxacpC7eZlodp3+dwkIgEaNTCflop+l35/pZ2ioGaIutZJbw8vhw4cpLCws84gnLCyMrVu3Vugce/bs4fbbb8eyLCzL4u6776Z79+6nPX7q1KlMmTKleLvozouIiJyDevXM5Hht2575OMuCjAxITTWvQ4fO/P7wYXA6OfU+yDM8wFg+YzkDWc5ANtCdVEr+hvhSsujmz/TmUy4v3k5gErfyBo/xNyI4WLbG/PySOs5FSMjpw02jRmZ/aGhJMDx1W3d+qk2te4jYv39/EhMTK3x8YGAggVpWVUSkZjkcJX/EO3Y8+/GFheaR1Cmhpm1qKm3T0hifthTS5pGRmsfu1AbkHT1O7nEnwWQVn+ImZhHPSo5Tn68ZwVeM4jXuZBY3cQ8vMo2pZcJRlWRmmteePef2eR8f12BztrBzuu2gIIWgU7g1vDRr1gxfX19SUlJc2lNSUgjXghoiInWXr6/pmNusGcTGnvawUCCuaCMvz3RGTkuDtDQuOPkiLY3/S1/M0k2/MPWnsSxPj2V/SCyOei0gLY2MwgbcywvEkcgwvqULW9wbairK6TTLgJdeCvxc+PiYWQMbNiz782zvmzSBgQPd8dvUKm4NLwEBAfTp04eFCxcWd+J1Op0sXLiQyZMnu/OrRETE2wUGlkyoV44LgaWWWZwyNnYsRI0Fy2LjgmxmjQguPq5t82xGxf7KyHZb6RO8jRYn9uOXkWZCxZEj5lX0vpyRsLZzOkvuAlVWZGT1raRuo0qHl6ysLJJKrfS1e/duEhMTadKkCW3atGHKlClMmDCBvn370r9/f1544QWys7OZOHGiWws/VUJCAgkJCRQWFp79YBER8QoOB4we7drQMiaYJ56AFSvghx9gz6EGvPpDV179oSsA770Hv/+9OXzZMnjllZPdWRpZNA7Op74zB9/8HHxzcxgas4eowANw9CjHkrNJSz5B89x9NDh+uCRQZGSUvD9+vMavwRk1bGh3BdWi0kOlFy9ezEUXXVSmfcKECcyaNQuAGTNmFE9SFxcXx0svvUR8fLxbCj4bzfMiIiJFsrNh8WJzd+abb8xyU999B0V/xmbOhNtvP/3nP/4YrrjCvP/f/+Dqq837jh1h8mS46aZT8sGJE2xenc2UqQHkZDm5ND6VcXG/0iE4uSTgnBp4ytt2VwiKj4effnLPuaqZ1ywPcC4UXkRE5HSKbs4XjYLesMGEmdJPkHJzzXGFhTB1Kpx3njn27bfhj390ndA4JMSsyn3nnSULWKammrUwS6/J2b07jBtnglCPHhXof5ufD8eOlYSZrCyzfexYyfvy2k5937evSV0eQOFF4UVERKqBZZmAM3s2vPgibN9u2vv2hdWrS4778EMzOnzuXPj++5LQBDB9Ovz5zzVbtydQeFF4ERGRauZ0wtdfwwsvmICyYwe0a1f2uPR0mD8fPvnEPLpasQLi4sy+jz82fXB+9zvz6tnTDC7KyDDhp+huDpgbMH5+ZuS0N1J4UXgREZFaKCvLde66W2+F//ynZH/9+mbAU9Ejp9xcM+gK4I474N//NuFlzBh49FHzOMpbVObvt08N1VTtEhISiI2NpV+/fnaXIiIiUq7gYNf+LnfdBdOmwahRpv/M8eMlwSU42Ny1KVK07FROjnks1aMHXHklrF9fc/XXFrrzIiIiUgsUFkJSkrn70ry5+VmaZZk+uNu2mX4z//ufaQO47z54/vmar9md6uSdFxEREU/m6wudOkGbNmWDC5g7NiEh0K+fufOyYQNcf71pL71KTmYm3HJLzdVth1q3tpGIiIicXdeu8P778MQTpiNvkaVLYd8+++qqCQovIiIiHqxTp7Lbjz9uTy01ReFFRETEi3To4DrE2ht5TZ8XjTYSERGpGzTaSERERGyn0UYiIiLitRReRERExKMovIiIiIhHUXgRERERj6LwIiIiIh7Fa8KLhkqLiIjUDRoqLSIiIrbTUGkRERHxWgovIiIi4lEUXkRERMSjKLyIiIiIR/G6VaWL+h9nZmbaXImIiIhUVNHf7YqMI/K68JKWlgZAZGSkzZWIiIhIZR07dozQ0NAzHuN14aVJkyYA7N2796y/vFRNZmYmkZGR7Nu3T8PSq5Guc83Rta4Zus41x5OutWVZHDt2jIiIiLMe63XhxcfHdOMJDQ2t9f9DeYuQkBBd6xqg61xzdK1rhq5zzfGUa13Rmw7qsCsiIiIeReFFREREPIrXhZfAwECeeOIJAgMD7S7F6+la1wxd55qja10zdJ1rjrdea69b20hERES8m9fdeRERERHvpvAiIiIiHkXhRURERDyKwouIiIh4FK8LLwkJCbRr14569eoRHx/PqlWr7C7Jo02bNo1+/frRsGFDWrRoweWXX862bdtcjsnNzWXSpEk0bdqU4OBgrrzySlJSUmyq2Ds8/fTTOBwO7r333uI2XWf32b9/P3/4wx9o2rQp9evXp3v37qxZs6Z4v2VZPP7447Rs2ZL69eszdOhQduzYYWPFnqmwsJDHHnuMqKgo6tevT3R0NH/7299c1q7Rta68JUuWcOmllxIREYHD4WDevHku+ytyTdPT0xk/fjwhISE0atSIW265haysrBr8LarI8iKzZ8+2AgICrDfffNPatGmTddttt1mNGjWyUlJS7C7NYw0fPtx66623rI0bN1qJiYnWqFGjrDZt2lhZWVnFx9xxxx1WZGSktXDhQmvNmjXWeeedZ51//vk2Vu3ZVq1aZbVr187q0aOHdc899xS36zq7R3p6utW2bVvrpptuslauXGnt2rXL+uabb6ykpKTiY55++mkrNDTUmjdvnrV+/Xpr7NixVlRUlHX8+HEbK/c8//jHP6ymTZta8+fPt3bv3m199NFHVnBwsPXiiy8WH6NrXXlffvml9cgjj1iffPKJBVhz58512V+RazpixAirZ8+e1k8//WQtXbrU6tChg3X99dfX8G9y7rwqvPTv39+aNGlS8XZhYaEVERFhTZs2zcaqvEtqaqoFWD/88INlWZZ19OhRy9/f3/roo4+Kj9myZYsFWCtWrLCrTI917NgxKyYmxvr222+tQYMGFYcXXWf3efDBB60LLrjgtPudTqcVHh5uTZ8+vbjt6NGjVmBgoPXBBx/URIleY/To0dbNN9/s0nbFFVdY48ePtyxL19odTg0vFbmmmzdvtgBr9erVxcd89dVXlsPhsPbv319jtVeF1zw2ys/PZ+3atQwdOrS4zcfHh6FDh7JixQobK/MuGRkZQMkCmGvXruXEiRMu171z5860adNG1/0cTJo0idGjR7tcT9B1dqfPPvuMvn37cvXVV9OiRQt69erFzJkzi/fv3r2b5ORkl2sdGhpKfHy8rnUlnX/++SxcuJDt27cDsH79epYtW8bIkSMBXevqUJFrumLFCho1akTfvn2Ljxk6dCg+Pj6sXLmyxms+F16zMOPhw4cpLCwkLCzMpT0sLIytW7faVJV3cTqd3HvvvQwcOJBu3boBkJycTEBAAI0aNXI5NiwsjOTkZBuq9FyzZ8/m559/ZvXq1WX26Tq7z65du3j11VeZMmUKDz/8MKtXr+b//u//CAgIYMKECcXXs7x/l+haV85DDz1EZmYmnTt3xtfXl8LCQv7xj38wfvx4AF3ralCRa5qcnEyLFi1c9vv5+dGkSROPue5eE16k+k2aNImNGzeybNkyu0vxOvv27eOee+7h22+/pV69enaX49WcTid9+/blqaeeAqBXr15s3LiR1157jQkTJthcnXf58MMPee+993j//ffp2rUriYmJ3HvvvUREROhaS5V4zWOjZs2a4evrW2b0RUpKCuHh4TZV5T0mT57M/Pnz+f7772ndunVxe3h4OPn5+Rw9etTleF33ylm7di2pqan07t0bPz8//Pz8+OGHH3jppZfw8/MjLCxM19lNWrZsSWxsrEtbly5d2Lt3L0Dx9dS/S6ru/vvv56GHHuK6666je/fu3HDDDdx3331MmzYN0LWuDhW5puHh4aSmprrsLygoID093WOuu9eEl4CAAPr06cPChQuL25xOJwsXLmTAgAE2VubZLMti8uTJzJ07l0WLFhEVFeWyv0+fPvj7+7tc923btrF3715d90oYMmQIGzZsIDExsfjVt29fxo8fX/xe19k9Bg4cWGa4//bt22nbti0AUVFRhIeHu1zrzMxMVq5cqWtdSTk5Ofj4uP6Z8fX1xel0ArrW1aEi13TAgAEcPXqUtWvXFh+zaNEinE4n8fHxNV7zObG7x7A7zZ492woMDLRmzZplbd682br99tutRo0aWcnJyXaX5rHuvPNOKzQ01Fq8eLF18ODB4ldOTk7xMXfccYfVpk0ba9GiRdaaNWusAQMGWAMGDLCxau9QerSRZek6u8uqVassPz8/6x//+Ie1Y8cO67333rOCgoKsd999t/iYp59+2mrUqJH16aefWr/88ot12WWXafjuOZgwYYLVqlWr4qHSn3zyidWsWTPrgQceKD5G17ryjh07Zq1bt85at26dBVjPP/+8tW7dOmvPnj2WZVXsmo4YMcLq1auXtXLlSmvZsmVWTEyMhkrb6eWXX7batGljBQQEWP3797d++uknu0vyaEC5r7feeqv4mOPHj1t33XWX1bhxYysoKMgaN26cdfDgQfuK9hKnhhddZ/f5/PPPrW7dulmBgYFW586drddff91lv9PptB577DErLCzMCgwMtIYMGWJt27bNpmo9V2ZmpnXPPfdYbdq0serVq2e1b9/eeuSRR6y8vLziY3StK+/7778v99/LEyZMsCyrYtc0LS3Nuv76663g4GArJCTEmjhxonXs2DEbfptz47CsUlMdioiIiNRyXtPnRUREROoGhRcRERHxKAovIiIi4lEUXkRERMSjKLyIiIiIR1F4EREREY+i8CIiIiIeReFFREREPIrCi4iIiHgUhRcRERHxKAovIiIi4lEUXkRERMSj/D9k/AQnvU6YRAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.0625159311350906e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 0.004874864903888249\n",
      "                   x: [ 1.195e+00  2.465e+01  2.441e+01  4.359e+00\n",
      "                        4.374e+00  1.972e+00  1.287e+00  1.000e-02\n",
      "                        1.083e+00  3.395e-01]\n",
      "                 nit: 384\n",
      "                nfev: 61830\n",
      "          population: [[ 1.195e+00  2.465e+01 ...  1.083e+00  3.395e-01]\n",
      "                       [ 1.115e+00  2.467e+01 ...  1.028e+00  2.123e-01]\n",
      "                       ...\n",
      "                       [ 1.045e+00  2.466e+01 ...  1.223e+00  3.704e-01]\n",
      "                       [ 1.209e+00  2.467e+01 ...  9.778e-01  2.522e-01]]\n",
      " population_energies: [ 4.875e-03  4.914e-03 ...  4.936e-03  4.913e-03]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSLE = 1.0534314175356835e-05\n",
      "             message: Optimization terminated successfully.\n",
      "             success: True\n",
      "                 fun: 0.0046776527606576255\n",
      "                   x: [ 1.865e+00  2.465e+01  2.441e+01  3.719e+00\n",
      "                        3.711e+00  2.005e+00  1.298e+00  8.281e-03\n",
      "                        7.311e-01  2.726e-01]\n",
      "                 nit: 429\n",
      "                nfev: 67687\n",
      "          population: [[ 1.865e+00  2.465e+01 ...  7.311e-01  2.726e-01]\n",
      "                       [ 1.847e+00  2.464e+01 ...  7.036e-01  2.322e-01]\n",
      "                       ...\n",
      "                       [ 1.869e+00  2.467e+01 ...  7.448e-01  2.906e-01]\n",
      "                       [ 1.701e+00  2.466e+01 ...  7.873e-01  2.782e-01]]\n",
      " population_energies: [ 4.678e-03  4.719e-03 ...  4.716e-03  4.711e-03]\n",
      "              constr: [array([ 0.000e+00])]\n",
      "    constr_violation: 0.0\n",
      "               maxcv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/home/gh464/miniconda3/envs/symbac/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:231: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "lowest_MSE_loss = 10\n",
    "while True:\n",
    "    for loss_fn in loss_fns:\n",
    "    \n",
    "        def error_function(params, *args):\n",
    "            synthetic = lorentzian_2d_DoG(*params, args[0])\n",
    "            return loss_fn(synthetic, args[1])\n",
    "    \n",
    "        #print(loss_fn)\n",
    "        small_fit_size = 50\n",
    "        result, MSE_loss_50px, fit_midline = fit_to_PSF(small_fit_size, lorentzian_2d_DoG, fit_lorentz, error_function)\n",
    "        #print(f\"50px fit = {MSE_loss_50px}\")\n",
    "        extrapolated_fit, MSE_loss_extrapolated, extrapolated_midline, real_PSF_midline = get_extrapolated_fit(result, small_fit_size, lorentzian_2d_DoG, error_function)\n",
    "        #print(f\"Extrapolated fit = {MSE_loss_extrapolated}\")\n",
    "        \n",
    "        results.append([result, MSE_loss_extrapolated])\n",
    "        if MSE_loss_extrapolated < lowest_MSE_loss:\n",
    "            print(f\"Lowest MSLE = {MSE_loss_extrapolated}\")\n",
    "            print(result)\n",
    "            lowest_MSE_loss = MSE_loss_extrapolated\n",
    "\n",
    "    \n",
    "            extrapolated_x = np.arange(-201/2, 201/2)\n",
    "            fit_x = np.arange(-len(fit_midline)/2, len(fit_midline)/2)\n",
    "        \n",
    "            \n",
    "            plt.plot(extrapolated_x, extrapolated_midline, c = \"red\", linewidth = 3)\n",
    "            plt.plot(fit_x, fit_midline, c = \"green\", linewidth = 3)\n",
    "            plt.plot(extrapolated_x, real_PSF_midline,  c = \"blue\", linestyle = \"--\")\n",
    "            plt.yscale(\"log\")\n",
    "            #plt.xscale(\"log\")\n",
    "            plt.xlim(0,)\n",
    "            plt.show()\n",
    "\n",
    "        #result_full_fit, MSE_loss_201px = fit_to_PSF(201, lorentzian_2d_DoG, fit_lorentz, error_function)\n",
    "        #print(f\"201px fit = {MSE_loss_201px}\")\n",
    "        #print(f\"Percentage difference = {perc_err(MSE_loss_extrapolated,MSE_loss_201px)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:symbac]",
   "language": "python",
   "name": "conda-env-symbac-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
